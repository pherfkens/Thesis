{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46ee2202-9786-41e9-a41c-8f09192e8c12",
   "metadata": {},
   "source": [
    "# Fine-Tuning the Audio Spectrogram Transformer (AST) for Audio Classification\n",
    "\n",
    "This Jupyter Notebook provides a comprehensive guide for fine-tuning the Audio Spectrogram Transformer (AST) model on your own audio classification dataset using tools from the HuggingFace ecosystem and PyTorch. The notebook covers the entire workflow, including data loading, preprocessing, applying audio augmentations, configuring the model, and setting up the training process.\n",
    "\n",
    "**Published:** 30.07.2024  \n",
    "**Author:** Marius Steger  \n",
    "**Email:** [marius.steger@renumics.com](mailto:marius.steger@renumics.com)  \n",
    "**Organization:** [Renumics](https://renumics.com/)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694e0e96-8bdf-4147-b990-9a6df1205fb2",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Packages\n",
    "Before we start, install all the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f43d402-4217-44be-a44a-def251c3e77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers[torch] datasets[audio] audiomentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172c2b9c-0b13-4cc0-bd1f-8b7c92b73cea",
   "metadata": {},
   "source": [
    "## Step 2: Load Your Data in the Correct Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99dafcdc-91e5-4bab-8ba1-699b79522238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, Audio, ClassLabel, Features, load_dataset\n",
    "import pandas as pd\n",
    "import os \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1721b140-c598-4bb5-b8f8-17664e0d085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define class labels\n",
    "# class_labels = ClassLabel(names=[\"bang\", \"dog_bark\"])\n",
    "\n",
    "# # Define features with audio and label columns\n",
    "# features = Features({\n",
    "#    \"audio\": Audio(),\n",
    "#    \"labels\": class_labels\n",
    "# })\n",
    "\n",
    "# # Load data (example with a dictionary)\n",
    "# dataset = Dataset.from_dict({\n",
    "#    \"audio\": [r\"C:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\data\\WAV_Groningen_1\\WAV_Groningen_1\\Noorderplantsoen\\NP101.wav\",\n",
    "#              r\"C:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\data\\WAV_Groningen_1\\WAV_Groningen_1\\Noorderplantsoen\\NP102.wav\"],\n",
    "#    \"labels\": [0, 1],\n",
    "# }, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d33f38c5-6b4a-4fd0-8398-ad08a4ffe011",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-existing dataset from the HuggingFace Hub\n",
    "esc50 = load_dataset(\"ashraq/esc50\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dde6ff68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LocationID</th>\n",
       "      <th>SessionID</th>\n",
       "      <th>GroupID</th>\n",
       "      <th>RecordID</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>Language</th>\n",
       "      <th>Survey_Version</th>\n",
       "      <th>...</th>\n",
       "      <th>RA_cp90</th>\n",
       "      <th>RA_cp95</th>\n",
       "      <th>THD_THD</th>\n",
       "      <th>THD_Min</th>\n",
       "      <th>THD_Max</th>\n",
       "      <th>THD_L5</th>\n",
       "      <th>THD_L10</th>\n",
       "      <th>THD_L50</th>\n",
       "      <th>THD_L90</th>\n",
       "      <th>THD_L95</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Noorderplantsoen</td>\n",
       "      <td>Noorderplantsoen1</td>\n",
       "      <td>NP101</td>\n",
       "      <td>2</td>\n",
       "      <td>2020-03-11 08:54:00</td>\n",
       "      <td>2020-03-11 09:04:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nld</td>\n",
       "      <td>nldSSIDv1</td>\n",
       "      <td>...</td>\n",
       "      <td>198.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-1312.0</td>\n",
       "      <td>5543.0</td>\n",
       "      <td>2294.0</td>\n",
       "      <td>1909.0</td>\n",
       "      <td>533.0</td>\n",
       "      <td>-993.0</td>\n",
       "      <td>-1104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Noorderplantsoen</td>\n",
       "      <td>Noorderplantsoen1</td>\n",
       "      <td>NP101</td>\n",
       "      <td>73</td>\n",
       "      <td>2020-03-13 00:49:00</td>\n",
       "      <td>2020-03-13 00:51:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nld</td>\n",
       "      <td>nldSSIDv1</td>\n",
       "      <td>...</td>\n",
       "      <td>198.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-1312.0</td>\n",
       "      <td>5543.0</td>\n",
       "      <td>2294.0</td>\n",
       "      <td>1909.0</td>\n",
       "      <td>533.0</td>\n",
       "      <td>-993.0</td>\n",
       "      <td>-1104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Noorderplantsoen</td>\n",
       "      <td>Noorderplantsoen1</td>\n",
       "      <td>NP102</td>\n",
       "      <td>88</td>\n",
       "      <td>2020-03-13 12:04:00</td>\n",
       "      <td>2020-03-13 12:08:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nld</td>\n",
       "      <td>nldSSIDv1</td>\n",
       "      <td>...</td>\n",
       "      <td>295.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>-275.0</td>\n",
       "      <td>-1402.0</td>\n",
       "      <td>6462.0</td>\n",
       "      <td>3921.0</td>\n",
       "      <td>323.0</td>\n",
       "      <td>1115.0</td>\n",
       "      <td>-1188.0</td>\n",
       "      <td>-126.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Noorderplantsoen</td>\n",
       "      <td>Noorderplantsoen1</td>\n",
       "      <td>NP103</td>\n",
       "      <td>89</td>\n",
       "      <td>2020-03-13 12:12:00</td>\n",
       "      <td>2020-03-13 12:14:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nld</td>\n",
       "      <td>nldSSIDv1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Noorderplantsoen</td>\n",
       "      <td>Noorderplantsoen1</td>\n",
       "      <td>NP106</td>\n",
       "      <td>98</td>\n",
       "      <td>2020-03-13 13:25:00</td>\n",
       "      <td>2020-03-13 13:32:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nld</td>\n",
       "      <td>nldSSIDv1</td>\n",
       "      <td>...</td>\n",
       "      <td>257.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>-624.0</td>\n",
       "      <td>-737.0</td>\n",
       "      <td>6889.0</td>\n",
       "      <td>2914.0</td>\n",
       "      <td>2397.0</td>\n",
       "      <td>969.0</td>\n",
       "      <td>-352.0</td>\n",
       "      <td>-447.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 142 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         LocationID          SessionID GroupID  RecordID          start_time  \\\n",
       "0  Noorderplantsoen  Noorderplantsoen1   NP101         2 2020-03-11 08:54:00   \n",
       "1  Noorderplantsoen  Noorderplantsoen1   NP101        73 2020-03-13 00:49:00   \n",
       "2  Noorderplantsoen  Noorderplantsoen1   NP102        88 2020-03-13 12:04:00   \n",
       "3  Noorderplantsoen  Noorderplantsoen1   NP103        89 2020-03-13 12:12:00   \n",
       "4  Noorderplantsoen  Noorderplantsoen1   NP106        98 2020-03-13 13:25:00   \n",
       "\n",
       "             end_time  latitude  longitude Language Survey_Version  ...  \\\n",
       "0 2020-03-11 09:04:00       NaN        NaN      nld      nldSSIDv1  ...   \n",
       "1 2020-03-13 00:51:00       NaN        NaN      nld      nldSSIDv1  ...   \n",
       "2 2020-03-13 12:08:00       NaN        NaN      nld      nldSSIDv1  ...   \n",
       "3 2020-03-13 12:14:00       NaN        NaN      nld      nldSSIDv1  ...   \n",
       "4 2020-03-13 13:32:00       NaN        NaN      nld      nldSSIDv1  ...   \n",
       "\n",
       "   RA_cp90  RA_cp95  THD_THD  THD_Min  THD_Max  THD_L5  THD_L10  THD_L50  \\\n",
       "0    198.0    152.0     -6.0  -1312.0   5543.0  2294.0   1909.0    533.0   \n",
       "1    198.0    152.0     -6.0  -1312.0   5543.0  2294.0   1909.0    533.0   \n",
       "2    295.0     23.0   -275.0  -1402.0   6462.0  3921.0    323.0   1115.0   \n",
       "3      NaN      NaN      NaN      NaN      NaN     NaN      NaN      NaN   \n",
       "4    257.0    203.0   -624.0   -737.0   6889.0  2914.0   2397.0    969.0   \n",
       "\n",
       "   THD_L90  THD_L95  \n",
       "0   -993.0  -1104.0  \n",
       "1   -993.0  -1104.0  \n",
       "2  -1188.0   -126.0  \n",
       "3      NaN      NaN  \n",
       "4   -352.0   -447.0  \n",
       "\n",
       "[5 rows x 142 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load metadata CSV (assuming dataset includes a CSV file linking audio to perceptual attributes)\n",
    "metadata = pd.read_excel(r\"C:\\Users\\pepij\\Downloads\\noorderplantsoen.xlsx\")\n",
    "\n",
    "# Preview dataset\n",
    "display(metadata.head())\n",
    "\n",
    "metadata[\"audio_path\"] = metadata[\"GroupID\"].apply(lambda x: r\"C:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\data\\WAV_Groningen_1\\WAV_Groningen_1\\Noorderplantsoen\\NP\" + x[2:] + \".wav\")\n",
    "\n",
    "# Keep only rows where the file exists\n",
    "metadata = metadata[metadata[\"audio_path\"].apply(os.path.exists)]\n",
    "\n",
    "# Reset index after filtering\n",
    "metadata.reset_index(drop=True, inplace=True)\n",
    "\n",
    "metadata = metadata[['GroupID', 'pleasant', 'chaotic', 'vibrant', 'uneventful', 'calm', 'annoying', 'eventful', 'monotonous', 'audio_path']]\n",
    "\n",
    "columns_to_convert = [\n",
    "    \"pleasant\", \"chaotic\", \"vibrant\", \"uneventful\", \n",
    "    \"calm\", \"annoying\", \"eventful\", \"monotonous\"\n",
    "]\n",
    "\n",
    "metadata[columns_to_convert] = metadata[columns_to_convert].astype(float).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f79f5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoundscapeDataset(Dataset):\n",
    "    def __init__(self, metadata):\n",
    "        self.metadata = metadata\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = self.metadata.iloc[idx][\"input_features\"]\n",
    "        labels = torch.tensor(self.metadata.iloc[idx][[\"pleasant\", \"vibrant\", \"eventful\", \"chaotic\", \n",
    "                                                       \"annoying\", \"monotonous\", \"uneventful\", \"calm\"]].values, dtype=torch.float32)\n",
    "        return features, labels\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "dataset = SoundscapeDataset(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcab0f01",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SoundscapeDataset' object has no attribute '_info'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\IPython\\core\\formatters.py:770\u001b[39m, in \u001b[36mPlainTextFormatter.__call__\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    763\u001b[39m stream = StringIO()\n\u001b[32m    764\u001b[39m printer = pretty.RepresentationPrinter(stream, \u001b[38;5;28mself\u001b[39m.verbose,\n\u001b[32m    765\u001b[39m     \u001b[38;5;28mself\u001b[39m.max_width, \u001b[38;5;28mself\u001b[39m.newline,\n\u001b[32m    766\u001b[39m     max_seq_length=\u001b[38;5;28mself\u001b[39m.max_seq_length,\n\u001b[32m    767\u001b[39m     singleton_pprinters=\u001b[38;5;28mself\u001b[39m.singleton_printers,\n\u001b[32m    768\u001b[39m     type_pprinters=\u001b[38;5;28mself\u001b[39m.type_printers,\n\u001b[32m    769\u001b[39m     deferred_pprinters=\u001b[38;5;28mself\u001b[39m.deferred_printers)\n\u001b[32m--> \u001b[39m\u001b[32m770\u001b[39m \u001b[43mprinter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpretty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    771\u001b[39m printer.flush()\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m stream.getvalue()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\IPython\\lib\\pretty.py:411\u001b[39m, in \u001b[36mRepresentationPrinter.pretty\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    400\u001b[39m                         \u001b[38;5;28;01mreturn\u001b[39;00m meth(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[32m    401\u001b[39m                 \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    402\u001b[39m                     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\n\u001b[32m    403\u001b[39m                     \u001b[38;5;66;03m# check if cls defines __repr__\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    409\u001b[39m                     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(_safe_getattr(\u001b[38;5;28mcls\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m__repr__\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    410\u001b[39m                 ):\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m                     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_repr_pprint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcycle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    413\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_pprint(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\IPython\\lib\\pretty.py:786\u001b[39m, in \u001b[36m_repr_pprint\u001b[39m\u001b[34m(obj, p, cycle)\u001b[39m\n\u001b[32m    784\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[39;00m\n\u001b[32m    785\u001b[39m \u001b[38;5;66;03m# Find newlines and replace them with p.break_()\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m786\u001b[39m output = \u001b[38;5;28mrepr\u001b[39m(obj)\n\u001b[32m    787\u001b[39m lines = output.splitlines()\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m p.group():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:2434\u001b[39m, in \u001b[36mDataset.__repr__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2433\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__repr__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m2434\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset(\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m    features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_info\u001b[49m.features.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m    num_rows: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.num_rows\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAttributeError\u001b[39m: 'SoundscapeDataset' object has no attribute '_info'"
     ]
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f52267-4482-4d68-bd26-c8a9b084fbfa",
   "metadata": {},
   "source": [
    "## Step 3: Preprocess the Audio Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71680e4d-cbfa-4a7d-8a2e-abaff8d5cb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import Audio, ClassLabel\n",
    "from transformers import ASTFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3cd728e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filename': Value(dtype='string', id=None),\n",
       " 'fold': Value(dtype='int64', id=None),\n",
       " 'target': Value(dtype='int64', id=None),\n",
       " 'category': Value(dtype='string', id=None),\n",
       " 'esc10': Value(dtype='bool', id=None),\n",
       " 'src_file': Value(dtype='int64', id=None),\n",
       " 'take': Value(dtype='string', id=None),\n",
       " 'audio': Audio(sampling_rate=None, mono=True, decode=True, id=None)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "esc50.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c593aecc-06c6-4220-b0dd-0463df5bead4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Column name ['target'] not in the dataset. Current columns in the dataset: ['filename', 'fold', 'labels', 'category', 'esc10', 'src_file', 'take', 'audio'].",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[94]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# get target value - class name mappings\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df = \u001b[43mesc50\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtarget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcategory\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m.to_pandas()\n\u001b[32m      3\u001b[39m class_names = df.iloc[np.unique(df[\u001b[33m\"\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m\"\u001b[39m], return_index=\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[32m1\u001b[39m]][\u001b[33m\"\u001b[39m\u001b[33mcategory\u001b[39m\u001b[33m\"\u001b[39m].to_list()\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(class_names)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:557\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    550\u001b[39m self_format = {\n\u001b[32m    551\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    552\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    553\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    554\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    555\u001b[39m }\n\u001b[32m    556\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    558\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\datasets\\fingerprint.py:442\u001b[39m, in \u001b[36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    438\u001b[39m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[32m    440\u001b[39m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:2342\u001b[39m, in \u001b[36mDataset.select_columns\u001b[39m\u001b[34m(self, column_names, new_fingerprint)\u001b[39m\n\u001b[32m   2340\u001b[39m missing_columns = \u001b[38;5;28mset\u001b[39m(column_names) - \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m._data.column_names)\n\u001b[32m   2341\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m missing_columns:\n\u001b[32m-> \u001b[39m\u001b[32m2342\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2343\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mColumn name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(missing_columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2344\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdataset. Current columns in the dataset: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2345\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._data.column_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2346\u001b[39m     )\n\u001b[32m   2348\u001b[39m dataset = copy.deepcopy(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m   2349\u001b[39m dataset._data = dataset._data.select(column_names)\n",
      "\u001b[31mValueError\u001b[39m: Column name ['target'] not in the dataset. Current columns in the dataset: ['filename', 'fold', 'labels', 'category', 'esc10', 'src_file', 'take', 'audio']."
     ]
    }
   ],
   "source": [
    "# get target value - class name mappings\n",
    "df = esc50.select_columns([\"target\", \"category\"]).to_pandas()\n",
    "class_names = df.iloc[np.unique(df[\"target\"], return_index=True)[1]][\"category\"].to_list()\n",
    "print(class_names)\n",
    "\n",
    "# cast target and audio column\n",
    "esc50 = esc50.cast_column(\"target\", ClassLabel(names=class_names))\n",
    "esc50 = esc50.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "# rename the target feature\n",
    "esc50 = esc50.rename_column(\"target\", \"labels\")\n",
    "num_labels = len(np.unique(esc50[\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7be7bc9e-5ba5-4289-9c28-71b82ddf9137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_values 16000\n"
     ]
    }
   ],
   "source": [
    "# Define the pretrained model and instantiate the feature extractor\n",
    "pretrained_model = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "feature_extractor = ASTFeatureExtractor.from_pretrained(pretrained_model)\n",
    "model_input_name = feature_extractor.model_input_names[0]\n",
    "SAMPLING_RATE = feature_extractor.sampling_rate\n",
    "\n",
    "print(model_input_name, SAMPLING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e636629e-6202-4a0c-a090-50023a6a829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_audio(batch):\n",
    "    wavs = [audio[\"array\"] for audio in batch[\"input_values\"]]\n",
    "    inputs = feature_extractor(wavs, sampling_rate=SAMPLING_RATE, return_tensors=\"pt\")\n",
    "    return {model_input_name: inputs.get(model_input_name), \"labels\": list(batch[\"labels\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "961de0e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['filename', 'fold', 'labels', 'category', 'esc10', 'src_file', 'take', 'audio'],\n",
       "    num_rows: 2000\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "esc50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "201eb68a-da4e-41ba-aa9f-20361bd1b26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the esc50 train split for this tutorial on how to fine-tune the AST Model\n",
    "dataset = esc50\n",
    "label2id = dataset.features[\"labels\"]._str2int  # we add the mapping from INTs to STRINGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a22d205a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': '1-100032-A-0.wav', 'fold': 1, 'labels': 0, 'category': 'dog', 'esc10': True, 'src_file': 100032, 'take': 'A', 'audio': {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83879a8a-a8f0-4a00-9ed5-63b2bd721d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training data\n",
    "if \"test\" not in dataset:\n",
    "    dataset = dataset.train_test_split(\n",
    "        test_size=0.2, shuffle=True, seed=0, stratify_by_column=\"labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54d506d-6908-478d-97d1-39f159940c73",
   "metadata": {},
   "source": [
    "## Step 4: Add Audio Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b43c7e36-e564-436d-958b-15b05c3bb94e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'audiomentations'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01maudiomentations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Compose, AddGaussianSNR, GainTransition, Gain, ClippingDistortion, TimeStretch, PitchShift\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'audiomentations'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from audiomentations import Compose, AddGaussianSNR, GainTransition, Gain, ClippingDistortion, TimeStretch, PitchShift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0c394b1-1900-411a-83d7-a1132bd3d2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define audio augmentations\n",
    "audio_augmentations = Compose([\n",
    "    AddGaussianSNR(min_snr_db=10, max_snr_db=20),\n",
    "    Gain(min_gain_db=-6, max_gain_db=6),\n",
    "    GainTransition(min_gain_db=-6, max_gain_db=6, min_duration=0.01, max_duration=0.3, duration_unit=\"fraction\"),\n",
    "    ClippingDistortion(min_percentile_threshold=0, max_percentile_threshold=30, p=0.5),\n",
    "    TimeStretch(min_rate=0.8, max_rate=1.2),\n",
    "    PitchShift(min_semitones=-4, max_semitones=4),\n",
    "], p=0.8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597c8e0e-1172-4dfd-90e8-d715b9eae1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing with augmentations\n",
    "def preprocess_audio_with_transforms(batch):\n",
    "    wavs = [audio_augmentations(audio[\"array\"], sample_rate=SAMPLING_RATE) for audio in batch[\"input_values\"]]\n",
    "    inputs = feature_extractor(wavs, sampling_rate=SAMPLING_RATE, return_tensors=\"pt\")\n",
    "    return {model_input_name: inputs.get(model_input_name), \"labels\": list(batch[\"labels\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4bad960a-13b5-43e9-89b9-902489af986c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=feature_extractor.sampling_rate))\n",
    "dataset = dataset.rename_column(\"audio\", \"input_values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2fea2fb-d150-4e8c-be60-3500adcea678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated mean and std: -3.3504603 4.387065\n"
     ]
    }
   ],
   "source": [
    "# calculate values for normalization\n",
    "feature_extractor.do_normalize = False  # we set normalization to False in order to calculate the mean + std of the dataset\n",
    "mean = []\n",
    "std = []\n",
    "\n",
    "# we use the transformation w/o augmentation on the training dataset to calculate the mean + std\n",
    "dataset[\"train\"].set_transform(preprocess_audio, output_all_columns=False)\n",
    "for i, (audio_input, labels) in enumerate(dataset[\"train\"]):\n",
    "    cur_mean = torch.mean(dataset[\"train\"][i][audio_input])\n",
    "    cur_std = torch.std(dataset[\"train\"][i][audio_input])\n",
    "    mean.append(cur_mean)\n",
    "    std.append(cur_std)\n",
    "\n",
    "feature_extractor.mean = np.mean(mean)\n",
    "feature_extractor.std = np.mean(std)\n",
    "feature_extractor.do_normalize = True\n",
    "\n",
    "print(\"Calculated mean and std:\", feature_extractor.mean, feature_extractor.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "030ab852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['test'].features['labels'].names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1d2ada71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2134, -0.6156, -0.2232,  ..., -0.1845, -0.2843, -0.6573],\n",
       "        [-0.0349, -0.3366,  0.0558,  ..., -0.2055, -0.3688, -0.6004],\n",
       "        [-0.1835, -0.3470,  0.0454,  ..., -0.2269, -0.2812, -0.5483],\n",
       "        ...,\n",
       "        [ 0.3819,  0.3819,  0.3819,  ...,  0.3819,  0.3819,  0.3819],\n",
       "        [ 0.3819,  0.3819,  0.3819,  ...,  0.3819,  0.3819,  0.3819],\n",
       "        [ 0.3819,  0.3819,  0.3819,  ...,  0.3819,  0.3819,  0.3819]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][0]['input_values']   #[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5898b2d0-0bcc-463e-8c27-96e7818111fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transforms\n",
    "# dataset[\"train\"].set_transform(preprocess_audio_with_transforms, output_all_columns=False)\n",
    "dataset[\"test\"].set_transform(preprocess_audio, output_all_columns=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b5447f-5add-46a1-8425-c094131cafd2",
   "metadata": {},
   "source": [
    "## Step 5: Configure and Initialize the AST for Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e49f002d-03b7-4e51-b6cd-1f4415f8b549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from transformers import ASTConfig, ASTForAudioClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d484ed33-d35d-4d3b-9b89-df37a0675ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration from the pretrained model\n",
    "config = ASTConfig.from_pretrained(pretrained_model)\n",
    "config.num_labels = num_labels\n",
    "config.label2id = label2id\n",
    "config.id2label = {v: k for k, v in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a400322c-e56a-4f95-8668-9307e32ce3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 and are newly initialized because the shapes did not match:\n",
      "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([50]) in the model instantiated\n",
      "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([50, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model with the updated configuration\n",
    "model = ASTForAudioClassification.from_pretrained(pretrained_model, config=config, ignore_mismatched_sizes=True)\n",
    "model.init_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee50f12a-47bc-4d31-93c1-989911e97037",
   "metadata": {},
   "source": [
    "### Setup Metrics and Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "866e564f-11ae-40ec-acb8-febee019ebe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./runs/ast_classifier\",\n",
    "    logging_dir=f\"./logs/ast_classifier\",\n",
    "    report_to=\"tensorboard\",\n",
    "    learning_rate=5e-5,  # LEARNING RATE\n",
    "    push_to_hub=False,\n",
    "    num_train_epochs=3,  # EPOCHS\n",
    "    per_device_train_batch_size=8,  # BATCH SIZE\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_steps=1,\n",
    "    save_steps=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",  # eval_+metric ist utilized\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "abb0e7cc-e3c0-46d2-8271-fa0eac9442c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.20k/4.20k [00:00<00:00, 2.14MB/s]\n",
      "Downloading builder script: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.38k/7.38k [00:00<?, ?B/s]\n",
      "Downloading builder script: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.56k/7.56k [00:00<?, ?B/s]\n",
      "Downloading builder script: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.79k/6.79k [00:00<?, ?B/s]\n"
     ]
    }
   ],
   "source": [
    "# Define evaluation metrics\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "AVERAGE = \"macro\" if config.num_labels > 2 else \"binary\"\n",
    "\n",
    "# setup metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    # get predictions and scores\n",
    "    logits = eval_pred.predictions\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "\n",
    "    # compute metrics\n",
    "    metrics = accuracy.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "    metrics.update(precision.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))\n",
    "    metrics.update(recall.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))\n",
    "    metrics.update(f1.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1f469b71-00e8-4f4e-ab24-434ebcda243d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,  # we use our configured training arguments\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,  # we the metrics function from above\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7af0820e-fa77-4f44-95ee-612896374c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  5/600 02:09 < 7:06:25, 0.02 it/s, Epoch 0.02/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# start a training\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2241\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2239\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2240\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2245\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2548\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2541\u001b[39m context = (\n\u001b[32m   2542\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2543\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2544\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2545\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2546\u001b[39m )\n\u001b[32m   2547\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2548\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2551\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2552\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2553\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2554\u001b[39m ):\n\u001b[32m   2555\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2556\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3698\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3695\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3697\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3698\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3700\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3701\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3702\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3703\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3704\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3759\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3757\u001b[39m         loss_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   3758\u001b[39m     inputs = {**inputs, **loss_kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m3759\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3760\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   3761\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   3762\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\transformers\\models\\audio_spectrogram_transformer\\modeling_audio_spectrogram_transformer.py:629\u001b[39m, in \u001b[36mASTForAudioClassification.forward\u001b[39m\u001b[34m(self, input_values, head_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    621\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    622\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[32m    623\u001b[39m \u001b[33;03m    Labels for computing the audio classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[32m    624\u001b[39m \u001b[33;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[32m    625\u001b[39m \u001b[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m    626\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    627\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m--> \u001b[39m\u001b[32m629\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maudio_spectrogram_transformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    637\u001b[39m pooled_output = outputs[\u001b[32m1\u001b[39m]\n\u001b[32m    638\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.classifier(pooled_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\transformers\\models\\audio_spectrogram_transformer\\modeling_audio_spectrogram_transformer.py:548\u001b[39m, in \u001b[36mASTModel.forward\u001b[39m\u001b[34m(self, input_values, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    544\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m    546\u001b[39m embedding_output = \u001b[38;5;28mself\u001b[39m.embeddings(input_values)\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    555\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    556\u001b[39m sequence_output = \u001b[38;5;28mself\u001b[39m.layernorm(sequence_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\transformers\\models\\audio_spectrogram_transformer\\modeling_audio_spectrogram_transformer.py:401\u001b[39m, in \u001b[36mASTEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    394\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    395\u001b[39m         layer_module.\u001b[34m__call__\u001b[39m,\n\u001b[32m    396\u001b[39m         hidden_states,\n\u001b[32m    397\u001b[39m         layer_head_mask,\n\u001b[32m    398\u001b[39m         output_attentions,\n\u001b[32m    399\u001b[39m     )\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    403\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\transformers\\models\\audio_spectrogram_transformer\\modeling_audio_spectrogram_transformer.py:345\u001b[39m, in \u001b[36mASTLayer.forward\u001b[39m\u001b[34m(self, hidden_states, head_mask, output_attentions)\u001b[39m\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    340\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    341\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m    342\u001b[39m     head_mask: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    343\u001b[39m     output_attentions: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    344\u001b[39m ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m     self_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayernorm_before\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# in AST, layernorm is applied before self-attention\u001b[39;49;00m\n\u001b[32m    347\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    350\u001b[39m     attention_output = self_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    351\u001b[39m     outputs = self_attention_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add self attentions if we output attention weights\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\transformers\\models\\audio_spectrogram_transformer\\modeling_audio_spectrogram_transformer.py:271\u001b[39m, in \u001b[36mASTAttention.forward\u001b[39m\u001b[34m(self, hidden_states, head_mask, output_attentions)\u001b[39m\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    266\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    267\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m    268\u001b[39m     head_mask: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    269\u001b[39m     output_attentions: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    270\u001b[39m ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m     self_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m     attention_output = \u001b[38;5;28mself\u001b[39m.output(self_outputs[\u001b[32m0\u001b[39m], hidden_states)\n\u001b[32m    275\u001b[39m     outputs = (attention_output,) + self_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\transformers\\models\\audio_spectrogram_transformer\\modeling_audio_spectrogram_transformer.py:197\u001b[39m, in \u001b[36mASTSdpaSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, head_mask, output_attentions)\u001b[39m\n\u001b[32m    185\u001b[39m     logger.warning_once(\n\u001b[32m    186\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`ASTSdpaAttention` is used but `torch.nn.functional.scaled_dot_product_attention` does not support \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    187\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    188\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mspecifying the manual implementation will be required from Transformers version v5.0.0 onwards. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    189\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mThis warning can be removed using the argument `attn_implementation=\u001b[39m\u001b[33m\"\u001b[39m\u001b[33meager\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m` when loading the model.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    190\u001b[39m     )\n\u001b[32m    191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().forward(\n\u001b[32m    192\u001b[39m         hidden_states=hidden_states,\n\u001b[32m    193\u001b[39m         head_mask=head_mask,\n\u001b[32m    194\u001b[39m         output_attentions=output_attentions,\n\u001b[32m    195\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m mixed_query_layer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m key_layer = \u001b[38;5;28mself\u001b[39m.transpose_for_scores(\u001b[38;5;28mself\u001b[39m.key(hidden_states))\n\u001b[32m    200\u001b[39m value_layer = \u001b[38;5;28mself\u001b[39m.transpose_for_scores(\u001b[38;5;28mself\u001b[39m.value(hidden_states))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# start a training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d12f99-709e-46a6-90f1-f982dcb27e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
