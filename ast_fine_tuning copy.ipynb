{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, Audio, ClassLabel, Features, load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from pathlib import Path\n",
    "source_dir = Path(r'C:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\data\\csv')\n",
    "path = source_dir.joinpath('first_try.csv')\n",
    "metadata = pd.read_csv(path)\n",
    "metadata\n",
    "paqs = [\n",
    "    \"pleasant\", \"chaotic\", \"vibrant\", \"uneventful\", \n",
    "    \"calm\", \"annoying\", \"eventful\", \"monotonous\"\n",
    "]\n",
    "\n",
    "# Before training â€” scale your labels\n",
    "metadata[paqs] = (metadata[paqs] / 5.0)\n",
    "metadata\n",
    "from datasets import Audio\n",
    "from transformers import ASTFeatureExtractor\n",
    "def load_audio(row):\n",
    "    waveform, sr = torchaudio.load(row['audio_path'])\n",
    "    return {\n",
    "        'filename': f\"{row['GroupID']}.wav\",\n",
    "        'labels': {k: row[k] for k in ['pleasant', 'chaotic', 'vibrant', 'uneventful', 'calm', 'annoying', 'eventful', 'monotonous']},\n",
    "        # 'labels': [\n",
    "        #     row['pleasant'], row['chaotic'], row['vibrant'], row['uneventful'],\n",
    "        #     row['calm'], row['annoying'], row['eventful'], row['monotonous']\n",
    "        # ],\n",
    "        'audio': {\n",
    "            'path': row['audio_path'],\n",
    "            'array': waveform.squeeze().numpy().reshape(-1),\n",
    "            'sampling_rate': sr\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Step 1: Apply transformation to entire DataFrame\n",
    "records = metadata.apply(load_audio, axis=1)\n",
    "\n",
    "# Step 2: Apply to rows and collect results as list of dicts\n",
    "records = [load_audio(row) for _, row in metadata.iterrows()]\n",
    "\n",
    "# Step 3: Create a new DataFrame directly\n",
    "new_df = pd.DataFrame(records)\n",
    "dataset = Dataset.from_pandas(new_df)\n",
    "new_df\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "dataset[0]\n",
    "dataset[0]['audio']['array'].shape\n",
    "# dataset[0]\n",
    "# De oude format als je eerst numpy array zou maken maar is dus niet nodig zie hierboven\n",
    "# Define the pretrained model and instantiate the feature extractor\n",
    "pretrained_model = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "feature_extractor = ASTFeatureExtractor.from_pretrained(pretrained_model)\n",
    "model_input_name = feature_extractor.model_input_names[0]\n",
    "SAMPLING_RATE = feature_extractor.sampling_rate\n",
    "label_keys = [\"pleasant\", \"chaotic\", \"vibrant\", \"uneventful\", \"calm\", \"annoying\", \"eventful\", \"monotonous\"]\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_audio(batch):\n",
    "    wavs = [audio[\"array\"] for audio in batch[\"input_values\"]]\n",
    "    inputs = feature_extractor(wavs, sampling_rate=SAMPLING_RATE, return_tensors=\"pt\")\n",
    "    labels = [[row[k] for k in label_keys] for row in batch[\"labels\"]]\n",
    "    return {model_input_name: inputs[model_input_name], \"labels\": torch.tensor(labels, dtype=torch.float32)}\n",
    "\n",
    "# split training data\n",
    "if \"test\" not in dataset:\n",
    "    dataset = dataset.train_test_split(\n",
    "        test_size=0.2, shuffle=True, seed=0)\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=feature_extractor.sampling_rate))\n",
    "dataset = dataset.rename_column(\"audio\", \"input_values\")\n",
    "# Apply transforms\n",
    "dataset[\"train\"].set_transform(preprocess_audio, output_all_columns=False)\n",
    "dataset[\"test\"].set_transform(preprocess_audio, output_all_columns=False)\n",
    "dataset['test'][0]\n",
    "import evaluate\n",
    "from transformers import ASTConfig, ASTForAudioClassification, TrainingArguments, Trainer\n",
    "# Load configuration from the pretrained model\n",
    "config = ASTConfig.from_pretrained(pretrained_model)\n",
    "\n",
    "# Modify the model's final layer for regression (8 outputs)\n",
    "model = ASTForAudioClassification.from_pretrained(pretrained_model, config=config, ignore_mismatched_sizes=True)\n",
    "model.classifier = nn.Linear(config.hidden_size, 8)\n",
    "model.init_weights()\n",
    "from transformers import Trainer\n",
    "import torch.nn as nn\n",
    "\n",
    "class RegressionTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss = nn.MSELoss()(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "# Configure training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./runs/ast_regressor_test\",\n",
    "    logging_dir=\"./logs/ast_regressor_test\",\n",
    "    report_to=\"none\",\n",
    "    learning_rate=5e-2,  # Learning rate\n",
    "    push_to_hub=False,\n",
    "    num_train_epochs=3,  # Number of epochs\n",
    "    per_device_train_batch_size=8,  # Batch size\n",
    "    eval_strategy=\"epoch\",  # Evaluate after each epoch\n",
    "    save_strategy=\"no\", # Set false to save time\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=False, # Set false to save time\n",
    "    metric_for_best_model=\"rmse\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=5, # Decrease to save time\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits = eval_pred.predictions\n",
    "    labels = eval_pred.label_ids\n",
    "\n",
    "    mse_value = mean_squared_error(labels, logits)\n",
    "    rmse_value = np.sqrt(mse_value)\n",
    "    mae_value = mean_absolute_error(labels, logits)\n",
    "    r2_value = r2_score(labels, logits)\n",
    "\n",
    "    return {\n",
    "        \"mse\": mse_value,\n",
    "        \"rmse\": rmse_value,\n",
    "        \"mae\": mae_value,\n",
    "        \"r2\": r2_value\n",
    "    }\n",
    "\n",
    "# OPTIONAL\n",
    "# If you ever want per-attribute RMSE for better model diagnostics:\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     logits = eval_pred.predictions\n",
    "#     labels = eval_pred.label_ids\n",
    "\n",
    "#     metrics = {}\n",
    "#     for i, name in enumerate([\"pleasant\", \"chaotic\", \"vibrant\", \"uneventful\", \"calm\", \"annoying\", \"eventful\", \"monotonous\"]):\n",
    "#         mse = mean_squared_error(labels[:, i], logits[:, i])\n",
    "#         rmse = np.sqrt(mse)\n",
    "#         metrics[f\"{name}_rmse\"] = rmse\n",
    "\n",
    "#     metrics[\"rmse\"] = np.sqrt(mean_squared_error(labels, logits))\n",
    "#     return metrics\n",
    "\n",
    "trainer = RegressionTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "trainer.predict(dataset[\"test\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
