{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu for training.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} for training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 and are newly initialized because the shapes did not match:\n",
      "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([8]) in the model instantiated\n",
      "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([8, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully with modified classifier!\n"
     ]
    }
   ],
   "source": [
    "from transformers import ASTForAudioClassification, ASTFeatureExtractor, ASTConfig\n",
    "# Modify the classifier layer to match your task\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load the feature extractor\n",
    "model_name = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "feature_extractor = ASTFeatureExtractor.from_pretrained(model_name)\n",
    "\n",
    "config = ASTConfig.from_pretrained(model_name)\n",
    "\n",
    "config.num_labels = 8  # Change the number of labels to match your task\n",
    "\n",
    "# Load AST model but ignore mismatched sizes\n",
    "model = ASTForAudioClassification.from_pretrained(\n",
    "    model_name,\n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "model_input_name = feature_extractor.model_input_names[0]\n",
    "\n",
    "model.init_weights()\n",
    "model.classifier = nn.Linear(model.config.hidden_size, 8)  # Replace classifier for 8 outputs\n",
    "model.init_weights()\n",
    "model.to(device) \n",
    "\n",
    "\n",
    "print(\"Model loaded successfully with modified classifier!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'input_values'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load metadata CSV (assuming dataset includes a CSV file linking audio to perceptual attributes)\n",
    "metadata = pd.read_excel(r\"C:\\Users\\pepij\\Downloads\\noorderplantsoen.xlsx\")\n",
    "\n",
    "# Preview dataset\n",
    "(metadata.head())\n",
    "\n",
    "metadata[\"audio_path\"] = metadata[\"GroupID\"].apply(lambda x: r\"C:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\data\\WAV_Groningen_1\\WAV_Groningen_1\\Noorderplantsoen\\NP\" + x[2:] + \".wav\")\n",
    "\n",
    "# Keep only rows where the file exists\n",
    "metadata = metadata[metadata[\"audio_path\"].apply(os.path.exists)]\n",
    "\n",
    "# Reset index after filtering\n",
    "metadata.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = metadata[['GroupID', 'pleasant', 'chaotic', 'vibrant', 'uneventful', 'calm', 'annoying', 'eventful', 'monotonous', 'audio_path']]\n",
    "\n",
    "columns_to_convert = [\n",
    "    \"pleasant\", \"chaotic\", \"vibrant\", \"uneventful\", \n",
    "    \"calm\", \"annoying\", \"eventful\", \"monotonous\"\n",
    "]\n",
    "\n",
    "metadata[columns_to_convert] = metadata[columns_to_convert].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "def preprocess_audio(audio_path):\n",
    "    waveform, sr = librosa.load(audio_path, sr=16000)  # Resample to 16kHz\n",
    "    # Explicitly normalize waveform\n",
    "    waveform = waveform / max(abs(waveform))  # Ensure range [-1,1]\n",
    "    features = feature_extractor(waveform, sampling_rate=sr, return_tensors=\"pt\")\n",
    "    return features.input_values.squeeze(0)\n",
    "\n",
    "# Apply preprocessing to dataset\n",
    "metadata[\"input_features\"] = metadata[\"audio_path\"].apply(preprocess_audio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(batch):\n",
    "    # Extract waveform data from dataset\n",
    "    wavs = [audio[\"array\"] for audio in batch[\"input_values\"]]  \n",
    "\n",
    "    # Convert waveforms into spectrograms using AST's feature extractor\n",
    "    inputs = feature_extractor(wavs, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Return formatted output with correct labels\n",
    "    output_batch = {\n",
    "        \"input_values\": inputs.input_values,  # Spectrograms as input\n",
    "        \"labels\": batch[\"labels\"],  # Keep labels unchanged\n",
    "    }\n",
    "    return output_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Original column name input_features not in the dataset. Current columns in the dataset: ['pleasant', 'chaotic', 'vibrant', 'uneventful', 'calm', 'annoying', 'eventful', 'monotonous', 'input_values']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[117]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Rename the audio column if necessary\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m hf_dataset = \u001b[43mhf_dataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrename_column\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_features\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_values\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Apply preprocessing to the dataset\u001b[39;00m\n\u001b[32m      5\u001b[39m hf_dataset.set_transform(preprocess_audio)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\datasets\\dataset_dict.py:415\u001b[39m, in \u001b[36mDatasetDict.rename_column\u001b[39m\u001b[34m(self, original_column_name, new_column_name)\u001b[39m\n\u001b[32m    377\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    378\u001b[39m \u001b[33;03mRename a column in the dataset and move the features associated to the original column under the new column name.\u001b[39;00m\n\u001b[32m    379\u001b[39m \u001b[33;03mThe transformation is applied to all the datasets of the dataset dictionary.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    411\u001b[39m \u001b[33;03m```\u001b[39;00m\n\u001b[32m    412\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    413\u001b[39m \u001b[38;5;28mself\u001b[39m._check_values_type()\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     \u001b[43m{\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrename_column\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m            \u001b[49m\u001b[43moriginal_column_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43moriginal_column_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnew_column_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_column_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m    422\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\datasets\\dataset_dict.py:416\u001b[39m, in \u001b[36m<dictcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    377\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    378\u001b[39m \u001b[33;03mRename a column in the dataset and move the features associated to the original column under the new column name.\u001b[39;00m\n\u001b[32m    379\u001b[39m \u001b[33;03mThe transformation is applied to all the datasets of the dataset dictionary.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    411\u001b[39m \u001b[33;03m```\u001b[39;00m\n\u001b[32m    412\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    413\u001b[39m \u001b[38;5;28mself\u001b[39m._check_values_type()\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[32m    415\u001b[39m     {\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m         k: \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrename_column\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m            \u001b[49m\u001b[43moriginal_column_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43moriginal_column_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnew_column_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_column_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    420\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.items()\n\u001b[32m    421\u001b[39m     }\n\u001b[32m    422\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\datasets\\fingerprint.py:442\u001b[39m, in \u001b[36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    438\u001b[39m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[32m    440\u001b[39m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:2208\u001b[39m, in \u001b[36mDataset.rename_column\u001b[39m\u001b[34m(self, original_column_name, new_column_name, new_fingerprint)\u001b[39m\n\u001b[32m   2206\u001b[39m dataset = copy.deepcopy(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m   2207\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m original_column_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m dataset._data.column_names:\n\u001b[32m-> \u001b[39m\u001b[32m2208\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2209\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOriginal column name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_column_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in the dataset. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2210\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCurrent columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset._data.column_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   2211\u001b[39m     )\n\u001b[32m   2212\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m new_column_name \u001b[38;5;129;01min\u001b[39;00m dataset._data.column_names:\n\u001b[32m   2213\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2214\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNew column name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_column_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m already in the dataset. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2215\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlease choose a column name which is not already in the dataset. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2216\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCurrent columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset._data.column_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   2217\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Original column name input_features not in the dataset. Current columns in the dataset: ['pleasant', 'chaotic', 'vibrant', 'uneventful', 'calm', 'annoying', 'eventful', 'monotonous', 'input_values']"
     ]
    }
   ],
   "source": [
    "# Rename the audio column if necessary\n",
    "hf_dataset = hf_dataset.rename_column(\"input_features\", \"input_values\")\n",
    "\n",
    "# Apply preprocessing to the dataset\n",
    "hf_dataset.set_transform(preprocess_audio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SoundscapeDataset(Dataset):\n",
    "    def __init__(self, metadata):\n",
    "        self.metadata = metadata\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Extract features\n",
    "        features = self.metadata.iloc[idx][\"input_features\"]\n",
    "\n",
    "        # Convert labels to float32 (fixes the error)\n",
    "        labels = self.metadata.iloc[idx][[\"pleasant\", \"vibrant\", \"eventful\", \"chaotic\", \n",
    "                                          \"annoying\", \"monotonous\", \"uneventful\", \"calm\"]].astype(float).values\n",
    "        labels = torch.tensor(labels, dtype=torch.float32)  # Convert to tensor\n",
    "\n",
    "        return features, labels\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "dataset = SoundscapeDataset(metadata)\n",
    "\n",
    "# Split into training & validation\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SoundscapeDataset.__init__ of <__main__.SoundscapeDataset object at 0x0000023B9EB8ACD0>>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.optim as optim\n",
    "# import torch.nn as nn\n",
    "\n",
    "# criterion = nn.MSELoss()  # Regression loss function\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=1e-5)  # Use AdamW optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 10  # Adjust based on performance\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "\n",
    "#     for batch in train_loader:\n",
    "#         inputs, targets = batch\n",
    "#         inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(inputs).logits  # Forward pass\n",
    "#         loss = criterion(outputs, targets)  # Compute loss\n",
    "\n",
    "#         loss.backward()  # Backpropagation\n",
    "#         optimizer.step()  # Update weights\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {total_loss/len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Configure training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./runs/ast_regressor\",\n",
    "    logging_dir=\"./logs/ast_regressor\",\n",
    "    report_to=\"tensorboard\",\n",
    "    learning_rate=5e-5,  # Learning rate\n",
    "    push_to_hub=False,\n",
    "    num_train_epochs=3,  # Number of epochs\n",
    "    per_device_train_batch_size=8,  # Batch size\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate after each epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rmse\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=20,\n",
    "    remove_unused_columns=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "mse = evaluate.load(\"mse\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits = eval_pred.predictions\n",
    "    labels = eval_pred.label_ids\n",
    "\n",
    "    mse_value = mse.compute(predictions=logits, references=labels)[\"mse\"]\n",
    "    rmse_value = np.sqrt(mse_value)  # Compute RMSE\n",
    "\n",
    "    return {\"mse\": mse_value, \"rmse\": rmse_value}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GroupID</th>\n",
       "      <th>pleasant</th>\n",
       "      <th>chaotic</th>\n",
       "      <th>vibrant</th>\n",
       "      <th>uneventful</th>\n",
       "      <th>calm</th>\n",
       "      <th>annoying</th>\n",
       "      <th>eventful</th>\n",
       "      <th>monotonous</th>\n",
       "      <th>audio_path</th>\n",
       "      <th>input_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NP102</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>C:\\Users\\pepij\\OneDrive - Delft University of ...</td>\n",
       "      <td>[[tensor(-0.7477), tensor(-1.0871), tensor(-0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NP136</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>C:\\Users\\pepij\\OneDrive - Delft University of ...</td>\n",
       "      <td>[[tensor(-1.1476), tensor(-1.2776), tensor(-0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NP153</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>C:\\Users\\pepij\\OneDrive - Delft University of ...</td>\n",
       "      <td>[[tensor(-1.0547), tensor(-1.2776), tensor(-1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NP146</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>C:\\Users\\pepij\\OneDrive - Delft University of ...</td>\n",
       "      <td>[[tensor(-0.9345), tensor(-1.2776), tensor(-0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NP116</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>C:\\Users\\pepij\\OneDrive - Delft University of ...</td>\n",
       "      <td>[[tensor(-0.9793), tensor(-1.2480), tensor(-0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  GroupID  pleasant  chaotic  vibrant  uneventful  calm  annoying  eventful  \\\n",
       "0   NP102       4.0      1.0      4.0         3.0   4.0       1.0       4.0   \n",
       "1   NP136       4.0      1.0      5.0         2.0   4.0       1.0       4.0   \n",
       "2   NP153       4.0      2.0      5.0         1.0   1.0       1.0       5.0   \n",
       "3   NP146       4.0      4.0      5.0         1.0   4.0       3.0       5.0   \n",
       "4   NP116       5.0      4.0      4.0         1.0   4.0       1.0       4.0   \n",
       "\n",
       "   monotonous                                         audio_path  \\\n",
       "0         1.0  C:\\Users\\pepij\\OneDrive - Delft University of ...   \n",
       "1         2.0  C:\\Users\\pepij\\OneDrive - Delft University of ...   \n",
       "2         1.0  C:\\Users\\pepij\\OneDrive - Delft University of ...   \n",
       "3         1.0  C:\\Users\\pepij\\OneDrive - Delft University of ...   \n",
       "4         1.0  C:\\Users\\pepij\\OneDrive - Delft University of ...   \n",
       "\n",
       "                                      input_features  \n",
       "0  [[tensor(-0.7477), tensor(-1.0871), tensor(-0....  \n",
       "1  [[tensor(-1.1476), tensor(-1.2776), tensor(-0....  \n",
       "2  [[tensor(-1.0547), tensor(-1.2776), tensor(-1....  \n",
       "3  [[tensor(-0.9345), tensor(-1.2776), tensor(-0....  \n",
       "4  [[tensor(-0.9793), tensor(-1.2480), tensor(-0....  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert Subset objects back to DataFrame\n",
    "train_df = metadata.iloc[train_dataset.indices].reset_index(drop=True)\n",
    "val_df = metadata.iloc[val_dataset.indices].reset_index(drop=True)\n",
    "\n",
    "display(train_df.head())\n",
    "\n",
    "# Ensure input_features is a list instead of a tensor\n",
    "train_df[\"input_features\"] = train_df[\"input_features\"].apply(lambda x: x.tolist() if isinstance(x, torch.Tensor) else x)\n",
    "val_df[\"input_features\"] = val_df[\"input_features\"].apply(lambda x: x.tolist() if isinstance(x, torch.Tensor) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GroupID</th>\n",
       "      <th>pleasant</th>\n",
       "      <th>chaotic</th>\n",
       "      <th>vibrant</th>\n",
       "      <th>uneventful</th>\n",
       "      <th>calm</th>\n",
       "      <th>annoying</th>\n",
       "      <th>eventful</th>\n",
       "      <th>monotonous</th>\n",
       "      <th>audio_path</th>\n",
       "      <th>input_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NP102</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>C:\\Users\\pepij\\OneDrive - Delft University of ...</td>\n",
       "      <td>[[-0.7477487325668335, -1.0871058702468872, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NP136</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>C:\\Users\\pepij\\OneDrive - Delft University of ...</td>\n",
       "      <td>[[-1.1475975513458252, -1.2775938510894775, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NP153</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>C:\\Users\\pepij\\OneDrive - Delft University of ...</td>\n",
       "      <td>[[-1.0547019243240356, -1.2775938510894775, -1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NP146</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>C:\\Users\\pepij\\OneDrive - Delft University of ...</td>\n",
       "      <td>[[-0.9345070123672485, -1.2775938510894775, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NP116</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>C:\\Users\\pepij\\OneDrive - Delft University of ...</td>\n",
       "      <td>[[-0.9792704582214355, -1.248040795326233, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>NP155</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>C:\\Users\\pepij\\OneDrive - Delft University of ...</td>\n",
       "      <td>[[-0.9310088753700256, -1.273632287979126, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>NP140</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>C:\\Users\\pepij\\OneDrive - Delft University of ...</td>\n",
       "      <td>[[-1.061864972114563, -1.1639467477798462, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>NP154</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>C:\\Users\\pepij\\OneDrive - Delft University of ...</td>\n",
       "      <td>[[-1.0211067199707031, -1.2775938510894775, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>NP133</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>C:\\Users\\pepij\\OneDrive - Delft University of ...</td>\n",
       "      <td>[[-0.9808656573295593, -1.2775938510894775, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>NP131</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>C:\\Users\\pepij\\OneDrive - Delft University of ...</td>\n",
       "      <td>[[-0.8377332091331482, -1.2775938510894775, -0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   GroupID  pleasant  chaotic  vibrant  uneventful  calm  annoying  eventful  \\\n",
       "0    NP102       4.0      1.0      4.0         3.0   4.0       1.0       4.0   \n",
       "1    NP136       4.0      1.0      5.0         2.0   4.0       1.0       4.0   \n",
       "2    NP153       4.0      2.0      5.0         1.0   1.0       1.0       5.0   \n",
       "3    NP146       4.0      4.0      5.0         1.0   4.0       3.0       5.0   \n",
       "4    NP116       5.0      4.0      4.0         1.0   4.0       1.0       4.0   \n",
       "..     ...       ...      ...      ...         ...   ...       ...       ...   \n",
       "57   NP155       4.0      3.0      5.0         1.0   2.0       2.0       4.0   \n",
       "58   NP140       4.0      4.0      5.0         2.0   4.0       2.0       4.0   \n",
       "59   NP154       3.0      4.0      5.0         1.0   4.0       1.0       4.0   \n",
       "60   NP133       3.0      3.0      4.0         3.0   2.0       2.0       3.0   \n",
       "61   NP131       5.0      2.0      4.0         3.0   4.0       2.0       4.0   \n",
       "\n",
       "    monotonous                                         audio_path  \\\n",
       "0          1.0  C:\\Users\\pepij\\OneDrive - Delft University of ...   \n",
       "1          2.0  C:\\Users\\pepij\\OneDrive - Delft University of ...   \n",
       "2          1.0  C:\\Users\\pepij\\OneDrive - Delft University of ...   \n",
       "3          1.0  C:\\Users\\pepij\\OneDrive - Delft University of ...   \n",
       "4          1.0  C:\\Users\\pepij\\OneDrive - Delft University of ...   \n",
       "..         ...                                                ...   \n",
       "57         1.0  C:\\Users\\pepij\\OneDrive - Delft University of ...   \n",
       "58         1.0  C:\\Users\\pepij\\OneDrive - Delft University of ...   \n",
       "59         1.0  C:\\Users\\pepij\\OneDrive - Delft University of ...   \n",
       "60         2.0  C:\\Users\\pepij\\OneDrive - Delft University of ...   \n",
       "61         2.0  C:\\Users\\pepij\\OneDrive - Delft University of ...   \n",
       "\n",
       "                                       input_features  \n",
       "0   [[-0.7477487325668335, -1.0871058702468872, -0...  \n",
       "1   [[-1.1475975513458252, -1.2775938510894775, -0...  \n",
       "2   [[-1.0547019243240356, -1.2775938510894775, -1...  \n",
       "3   [[-0.9345070123672485, -1.2775938510894775, -0...  \n",
       "4   [[-0.9792704582214355, -1.248040795326233, -0....  \n",
       "..                                                ...  \n",
       "57  [[-0.9310088753700256, -1.273632287979126, -0....  \n",
       "58  [[-1.061864972114563, -1.1639467477798462, -0....  \n",
       "59  [[-1.0211067199707031, -1.2775938510894775, -0...  \n",
       "60  [[-0.9808656573295593, -1.2775938510894775, -0...  \n",
       "61  [[-0.8377332091331482, -1.2775938510894775, -0...  \n",
       "\n",
       "[62 rows x 11 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "hf_dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df),\n",
    "    \"test\": Dataset.from_pandas(val_df),\n",
    "})\n",
    "\n",
    "# Remove unnecessary columns (adjust based on your dataset)\n",
    "hf_dataset = hf_dataset.remove_columns([\"audio_path\", \"GroupID\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 62/62 [00:04<00:00, 12.81 examples/s]\n",
      "Map: 100%|██████████| 16/16 [00:01<00:00, 12.53 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert `input_features` from lists to PyTorch tensors\n",
    "def convert_to_tensor(example):\n",
    "    example[\"input_features\"] = torch.tensor(example[\"input_features\"], dtype=torch.float32)\n",
    "    return example\n",
    "\n",
    "hf_dataset = hf_dataset.map(convert_to_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import default_collate\n",
    "\n",
    "# Custom data collator for AST\n",
    "def data_collator(features):\n",
    "    input_values = torch.stack([torch.tensor(f[\"input_features\"], dtype=torch.float32) for f in features])\n",
    "    labels = torch.stack([torch.tensor([f[col] for col in [\"pleasant\", \"vibrant\", \"eventful\", \n",
    "                                                           \"chaotic\", \"annoying\", \"monotonous\", \n",
    "                                                           \"uneventful\", \"calm\"]], dtype=torch.float32) \n",
    "                          for f in features])\n",
    "    return {\"input_values\": input_values, \"labels\": labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=hf_dataset[\"train\"],\n",
    "    eval_dataset=hf_dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    # data_collator=data_collator  # Use the corrected collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ASTForAudioClassification.forward() got an unexpected keyword argument 'pleasant'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[94]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2241\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2239\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2240\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2245\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2548\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2541\u001b[39m context = (\n\u001b[32m   2542\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2543\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2544\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2545\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2546\u001b[39m )\n\u001b[32m   2547\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2548\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2551\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2552\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2553\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2554\u001b[39m ):\n\u001b[32m   2555\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2556\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3698\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3695\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3697\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3698\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3700\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3701\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3702\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3703\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3704\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3759\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3757\u001b[39m         loss_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   3758\u001b[39m     inputs = {**inputs, **loss_kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m3759\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3760\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   3761\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   3762\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mTypeError\u001b[39m: ASTForAudioClassification.forward() got an unexpected keyword argument 'pleasant'"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['pleasant', 'chaotic', 'vibrant', 'uneventful', 'calm', 'annoying', 'eventful', 'monotonous', 'input_features'],\n",
       "        num_rows: 62\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['pleasant', 'chaotic', 'vibrant', 'uneventful', 'calm', 'annoying', 'eventful', 'monotonous', 'input_features'],\n",
       "        num_rows: 16\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(dtype='float64', id=None)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset[\"test\"].features['pleasant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(dtype='float64', id=None)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset[\"test\"].features['vibrant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
