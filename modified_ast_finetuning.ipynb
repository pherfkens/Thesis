{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46ee2202-9786-41e9-a41c-8f09192e8c12",
   "metadata": {},
   "source": [
    "# Fine-Tuning the Audio Spectrogram Transformer (AST) for Audio Classification\n",
    "\n",
    "This Jupyter Notebook provides a comprehensive guide for fine-tuning the Audio Spectrogram Transformer (AST) model on your own audio classification dataset using tools from the HuggingFace ecosystem and PyTorch. The notebook covers the entire workflow, including data loading, preprocessing, applying audio augmentations, configuring the model, and setting up the training process.\n",
    "\n",
    "**Published:** 30.07.2024  \n",
    "**Author:** Marius Steger  \n",
    "**Email:** [marius.steger@renumics.com](mailto:marius.steger@renumics.com)  \n",
    "**Organization:** [Renumics](https://renumics.com/)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694e0e96-8bdf-4147-b990-9a6df1205fb2",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Packages\n",
    "Before we start, install all the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f43d402-4217-44be-a44a-def251c3e77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: audiomentations in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (0.36.0)\n",
      "Requirement already satisfied: transformers[torch] in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (4.43.1)\n",
      "Requirement already satisfied: datasets[audio] in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (2.19.1)\n",
      "Requirement already satisfied: filelock in /home/marius/.local/lib/python3.11/site-packages (from transformers[torch]) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from transformers[torch]) (0.24.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from transformers[torch]) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from transformers[torch]) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from transformers[torch]) (2024.5.15)\n",
      "Requirement already satisfied: requests in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from transformers[torch]) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from transformers[torch]) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from transformers[torch]) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from transformers[torch]) (4.66.4)\n",
      "Requirement already satisfied: torch in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from transformers[torch]) (2.3.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from transformers[torch]) (0.33.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from datasets[audio]) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from datasets[audio]) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from datasets[audio]) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from datasets[audio]) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from datasets[audio]) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets[audio]) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from datasets[audio]) (3.9.5)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from datasets[audio]) (0.12.1)\n",
      "Requirement already satisfied: librosa in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from datasets[audio]) (0.10.2.post1)\n",
      "Requirement already satisfied: scipy<1.13,>=1.4 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from audiomentations) (1.12.0)\n",
      "Requirement already satisfied: soxr<1.0.0,>=0.3.2 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from audiomentations) (0.3.7)\n",
      "Requirement already satisfied: psutil in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from accelerate>=0.21.0->transformers[torch]) (6.0.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from aiohttp->datasets[audio]) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from aiohttp->datasets[audio]) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from aiohttp->datasets[audio]) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from aiohttp->datasets[audio]) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from aiohttp->datasets[audio]) (1.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.11.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from librosa->datasets[audio]) (3.0.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from librosa->datasets[audio]) (1.5.1)\n",
      "Requirement already satisfied: joblib>=0.14 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from librosa->datasets[audio]) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from librosa->datasets[audio]) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from librosa->datasets[audio]) (0.60.0)\n",
      "Requirement already satisfied: pooch>=1.1 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from librosa->datasets[audio]) (1.8.2)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from librosa->datasets[audio]) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from librosa->datasets[audio]) (1.0.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from requests->transformers[torch]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from requests->transformers[torch]) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from requests->transformers[torch]) (2024.7.4)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from soundfile>=0.12.1->datasets[audio]) (1.16.0)\n",
      "Requirement already satisfied: sympy in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from torch->transformers[torch]) (1.13.1)\n",
      "Requirement already satisfied: networkx in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from torch->transformers[torch]) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from torch->transformers[torch]) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from torch->transformers[torch]) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from torch->transformers[torch]) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from torch->transformers[torch]) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from torch->transformers[torch]) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from torch->transformers[torch]) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from torch->transformers[torch]) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from torch->transformers[torch]) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from torch->transformers[torch]) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.5.82)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from pandas->datasets[audio]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from pandas->datasets[audio]) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from pandas->datasets[audio]) (2023.3)\n",
      "Requirement already satisfied: pycparser in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from cffi>=1.0->soundfile>=0.12.1->datasets[audio]) (2.22)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from numba>=0.51.0->librosa->datasets[audio]) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /home/marius/.local/lib/python3.11/site-packages (from pooch>=1.1->librosa->datasets[audio]) (4.2.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets[audio]) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from scikit-learn>=0.20.0->librosa->datasets[audio]) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers[torch] datasets[audio] audiomentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172c2b9c-0b13-4cc0-bd1f-8b7c92b73cea",
   "metadata": {},
   "source": [
    "## Step 2: Load Your Data in the Correct Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99dafcdc-91e5-4bab-8ba1-699b79522238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, Audio, ClassLabel, Features, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1721b140-c598-4bb5-b8f8-17664e0d085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class labels\n",
    "#class_labels = ClassLabel(names=[\"bang\", \"dog_bark\"])\n",
    "\n",
    "# Define features with audio and label columns\n",
    "#features = Features({\n",
    "#    \"audio\": Audio(),\n",
    "#    \"labels\": class_labels\n",
    "#})\n",
    "\n",
    "# Load data (example with a dictionary)\n",
    "#dataset = Dataset.from_dict({\n",
    "#    \"audio\": [\"/audio/fold1/7061-6-0-0.wav\", \"/audio/fold1/7383-3-0-0.wav\"],\n",
    "#    \"labels\": [0, 1],\n",
    "#}, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d33f38c5-6b4a-4fd0-8398-ad08a4ffe011",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load a pre-existing dataset from the HuggingFace Hub\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m esc50 = \u001b[43mload_dataset\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mashraq/esc50\u001b[39m\u001b[33m\"\u001b[39m, split=\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'load_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Load a pre-existing dataset from the HuggingFace Hub\n",
    "esc50 = load_dataset(\"ashraq/esc50\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f52267-4482-4d68-bd26-c8a9b084fbfa",
   "metadata": {},
   "source": [
    "## Step 3: Preprocess the Audio Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71680e4d-cbfa-4a7d-8a2e-abaff8d5cb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import Audio, ClassLabel\n",
    "from transformers import ASTFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c593aecc-06c6-4220-b0dd-0463df5bead4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get target value - class name mappings\n",
    "df = esc50.select_columns([\"target\", \"category\"]).to_pandas()\n",
    "class_names = df.iloc[np.unique(df[\"target\"], return_index=True)[1]][\"category\"].to_list()\n",
    "\n",
    "# cast target and audio column\n",
    "esc50 = esc50.cast_column(\"target\", ClassLabel(names=class_names))\n",
    "esc50 = esc50.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "# rename the target feature\n",
    "esc50 = esc50.rename_column(\"target\", \"labels\")\n",
    "num_labels = len(np.unique(esc50[\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7be7bc9e-5ba5-4289-9c28-71b82ddf9137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pretrained model and instantiate the feature extractor\n",
    "pretrained_model = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "feature_extractor = ASTFeatureExtractor.from_pretrained(pretrained_model)\n",
    "model_input_name = feature_extractor.model_input_names[0]\n",
    "SAMPLING_RATE = feature_extractor.sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e636629e-6202-4a0c-a090-50023a6a829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_audio(batch):\n",
    "    wavs = [audio[\"array\"] for audio in batch[\"input_values\"]]\n",
    "    inputs = feature_extractor(wavs, sampling_rate=SAMPLING_RATE, return_tensors=\"pt\")\n",
    "    return {model_input_name: inputs.get(model_input_name), \"labels\": list(batch[\"labels\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "201eb68a-da4e-41ba-aa9f-20361bd1b26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the esc50 train split for this tutorial on how to fine-tune the AST Model\n",
    "dataset = esc50\n",
    "label2id = dataset.features[\"labels\"]._str2int  # we add the mapping from INTs to STRINGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83879a8a-a8f0-4a00-9ed5-63b2bd721d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training data\n",
    "if \"test\" not in dataset:\n",
    "    dataset = dataset.train_test_split(\n",
    "        test_size=0.2, shuffle=True, seed=0, stratify_by_column=\"labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54d506d-6908-478d-97d1-39f159940c73",
   "metadata": {},
   "source": [
    "## Step 4: Add Audio Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b43c7e36-e564-436d-958b-15b05c3bb94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from audiomentations import Compose, AddGaussianSNR, GainTransition, Gain, ClippingDistortion, TimeStretch, PitchShift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0c394b1-1900-411a-83d7-a1132bd3d2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define audio augmentations\n",
    "audio_augmentations = Compose([\n",
    "    AddGaussianSNR(min_snr_db=10, max_snr_db=20),\n",
    "    Gain(min_gain_db=-6, max_gain_db=6),\n",
    "    GainTransition(min_gain_db=-6, max_gain_db=6, min_duration=0.01, max_duration=0.3, duration_unit=\"fraction\"),\n",
    "    ClippingDistortion(min_percentile_threshold=0, max_percentile_threshold=30, p=0.5),\n",
    "    TimeStretch(min_rate=0.8, max_rate=1.2),\n",
    "    PitchShift(min_semitones=-4, max_semitones=4),\n",
    "], p=0.8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "597c8e0e-1172-4dfd-90e8-d715b9eae1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing with augmentations\n",
    "def preprocess_audio_with_transforms(batch):\n",
    "    wavs = [audio_augmentations(audio[\"array\"], sample_rate=SAMPLING_RATE) for audio in batch[\"input_values\"]]\n",
    "    inputs = feature_extractor(wavs, sampling_rate=SAMPLING_RATE, return_tensors=\"pt\")\n",
    "    return {model_input_name: inputs.get(model_input_name), \"labels\": list(batch[\"labels\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bad960a-13b5-43e9-89b9-902489af986c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=feature_extractor.sampling_rate))\n",
    "dataset = dataset.rename_column(\"audio\", \"input_values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2fea2fb-d150-4e8c-be60-3500adcea678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated mean and std: -3.3504603 4.387065\n"
     ]
    }
   ],
   "source": [
    "# calculate values for normalization\n",
    "feature_extractor.do_normalize = False  # we set normalization to False in order to calculate the mean + std of the dataset\n",
    "mean = []\n",
    "std = []\n",
    "\n",
    "# we use the transformation w/o augmentation on the training dataset to calculate the mean + std\n",
    "dataset[\"train\"].set_transform(preprocess_audio, output_all_columns=False)\n",
    "for i, (audio_input, labels) in enumerate(dataset[\"train\"]):\n",
    "    cur_mean = torch.mean(dataset[\"train\"][i][audio_input])\n",
    "    cur_std = torch.std(dataset[\"train\"][i][audio_input])\n",
    "    mean.append(cur_mean)\n",
    "    std.append(cur_std)\n",
    "\n",
    "feature_extractor.mean = np.mean(mean)\n",
    "feature_extractor.std = np.mean(std)\n",
    "feature_extractor.do_normalize = True\n",
    "\n",
    "print(\"Calculated mean and std:\", feature_extractor.mean, feature_extractor.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5898b2d0-0bcc-463e-8c27-96e7818111fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transforms\n",
    "dataset[\"train\"].set_transform(preprocess_audio_with_transforms, output_all_columns=False)\n",
    "dataset[\"test\"].set_transform(preprocess_audio, output_all_columns=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b5447f-5add-46a1-8425-c094131cafd2",
   "metadata": {},
   "source": [
    "## Step 5: Configure and Initialize the AST for Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e49f002d-03b7-4e51-b6cd-1f4415f8b549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from transformers import ASTConfig, ASTForAudioClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d484ed33-d35d-4d3b-9b89-df37a0675ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration from the pretrained model\n",
    "config = ASTConfig.from_pretrained(pretrained_model)\n",
    "config.num_labels = num_labels\n",
    "config.label2id = label2id\n",
    "config.id2label = {v: k for k, v in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a400322c-e56a-4f95-8668-9307e32ce3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 and are newly initialized because the shapes did not match:\n",
      "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([50]) in the model instantiated\n",
      "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([50, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model with the updated configuration\n",
    "model = ASTForAudioClassification.from_pretrained(pretrained_model, config=config, ignore_mismatched_sizes=True)\n",
    "model.init_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee50f12a-47bc-4d31-93c1-989911e97037",
   "metadata": {},
   "source": [
    "### Setup Metrics and Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "866e564f-11ae-40ec-acb8-febee019ebe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./runs/ast_classifier\",\n",
    "    logging_dir=f\"./logs/ast_classifier\",\n",
    "    report_to=\"tensorboard\",\n",
    "    learning_rate=5e-5,  # LEARNING RATE\n",
    "    push_to_hub=False,\n",
    "    num_train_epochs=10,  # EPOCHS\n",
    "    per_device_train_batch_size=8,  # BATCH SIZE\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_steps=1,\n",
    "    save_steps=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",  # eval_+metric ist utilized\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abb0e7cc-e3c0-46d2-8271-fa0eac9442c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metrics\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "AVERAGE = \"macro\" if config.num_labels > 2 else \"binary\"\n",
    "\n",
    "# setup metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    # get predictions and scores\n",
    "    logits = eval_pred.predictions\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "\n",
    "    # compute metrics\n",
    "    metrics = accuracy.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "    metrics.update(precision.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))\n",
    "    metrics.update(recall.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))\n",
    "    metrics.update(f1.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f469b71-00e8-4f4e-ab24-434ebcda243d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,  # we use our configured training arguments\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,  # we the metrics function from above\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7af0820e-fa77-4f44-95ee-612896374c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages/audiomentations/core/transforms_interface.py:62: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2000/2000 12:14, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.064900</td>\n",
       "      <td>0.557234</td>\n",
       "      <td>0.852500</td>\n",
       "      <td>0.888413</td>\n",
       "      <td>0.852500</td>\n",
       "      <td>0.844813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.470500</td>\n",
       "      <td>0.292247</td>\n",
       "      <td>0.902500</td>\n",
       "      <td>0.918432</td>\n",
       "      <td>0.902500</td>\n",
       "      <td>0.902523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.402400</td>\n",
       "      <td>0.295535</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.935807</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.921613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.402100</td>\n",
       "      <td>0.304958</td>\n",
       "      <td>0.927500</td>\n",
       "      <td>0.938123</td>\n",
       "      <td>0.927500</td>\n",
       "      <td>0.926955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.229600</td>\n",
       "      <td>0.268698</td>\n",
       "      <td>0.915000</td>\n",
       "      <td>0.925730</td>\n",
       "      <td>0.915000</td>\n",
       "      <td>0.911636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.225600</td>\n",
       "      <td>0.191082</td>\n",
       "      <td>0.947500</td>\n",
       "      <td>0.953143</td>\n",
       "      <td>0.947500</td>\n",
       "      <td>0.947632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.178400</td>\n",
       "      <td>0.268304</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.938053</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.925158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.080200</td>\n",
       "      <td>0.175834</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.956268</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.950184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.082000</td>\n",
       "      <td>0.141045</td>\n",
       "      <td>0.957500</td>\n",
       "      <td>0.962000</td>\n",
       "      <td>0.957500</td>\n",
       "      <td>0.957410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>0.133586</td>\n",
       "      <td>0.955000</td>\n",
       "      <td>0.959500</td>\n",
       "      <td>0.955000</td>\n",
       "      <td>0.955100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 1024}\n",
      "/home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages/audiomentations/core/transforms_interface.py:62: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 1024}\n",
      "/home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages/audiomentations/core/transforms_interface.py:62: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 1024}\n",
      "/home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages/audiomentations/core/transforms_interface.py:62: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 1024}\n",
      "/home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages/audiomentations/core/transforms_interface.py:62: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 1024}\n",
      "/home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages/audiomentations/core/transforms_interface.py:62: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 1024}\n",
      "/home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages/audiomentations/core/transforms_interface.py:62: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 1024}\n",
      "/home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages/audiomentations/core/transforms_interface.py:62: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 1024}\n",
      "/home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages/audiomentations/core/transforms_interface.py:62: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 1024}\n",
      "/home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages/audiomentations/core/transforms_interface.py:62: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 1024}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 1024}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2000, training_loss=0.4032806022465229, metrics={'train_runtime': 734.9587, 'train_samples_per_second': 21.77, 'train_steps_per_second': 2.721, 'total_flos': 1.084989898752e+18, 'train_loss': 0.4032806022465229, 'epoch': 10.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start a training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d12f99-709e-46a6-90f1-f982dcb27e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e24fd51",
   "metadata": {},
   "source": [
    "## Loading the Dataset for Perceptual Attributes Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba74c661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/data/noorderplantsoen.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load the Excel dataset\u001b[39;00m\n\u001b[32m      5\u001b[39m file_path = \u001b[33m\"\u001b[39m\u001b[33m/mnt/data/noorderplantsoen.xlsx\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSheet1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Select relevant columns (perceptual attributes)\u001b[39;00m\n\u001b[32m      9\u001b[39m perceptual_attributes = [\u001b[33m\"\u001b[39m\u001b[33mpleasant\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mchaotic\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mvibrant\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33muneventful\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcalm\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mannoying\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33meventful\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmonotonous\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:495\u001b[39m, in \u001b[36mread_excel\u001b[39m\u001b[34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[39m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[32m    494\u001b[39m     should_close = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m     io = \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine != io.engine:\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    503\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mEngine should not be specified when passing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    505\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1550\u001b[39m, in \u001b[36mExcelFile.__init__\u001b[39m\u001b[34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m   1548\u001b[39m     ext = \u001b[33m\"\u001b[39m\u001b[33mxls\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1550\u001b[39m     ext = \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1551\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m   1552\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1553\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1554\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1555\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mExcel file format cannot be determined, you must specify \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1556\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33man engine manually.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1557\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1402\u001b[39m, in \u001b[36minspect_excel_format\u001b[39m\u001b[34m(content_or_path, storage_options)\u001b[39m\n\u001b[32m   1399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m   1400\u001b[39m     content_or_path = BytesIO(content_or_path)\n\u001b[32m-> \u001b[39m\u001b[32m1402\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m   1404\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[32m   1405\u001b[39m     stream = handle.handle\n\u001b[32m   1406\u001b[39m     stream.seek(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/mnt/data/noorderplantsoen.xlsx'"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the Excel dataset\n",
    "file_path = \"/mnt/data/noorderplantsoen.xlsx\"\n",
    "df = pd.read_excel(file_path, sheet_name=\"Sheet1\")\n",
    "\n",
    "# Select relevant columns (perceptual attributes)\n",
    "perceptual_attributes = [\"pleasant\", \"chaotic\", \"vibrant\", \"uneventful\", \"calm\", \"annoying\", \"eventful\", \"monotonous\"]\n",
    "df = df[perceptual_attributes]\n",
    "\n",
    "# Convert to Hugging Face dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "print(dataset)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a85681",
   "metadata": {},
   "source": [
    "## Modifying AST Model for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94056f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import ASTForAudioClassification, AutoConfig\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load AST model and configuration\n",
    "model_name = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "# Modify the model's final layer for regression (8 outputs)\n",
    "model = ASTForAudioClassification.from_pretrained(model_name, config=config)\n",
    "model.classifier = nn.Linear(config.hidden_size, len(perceptual_attributes))\n",
    "\n",
    "print(model)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61319e9e",
   "metadata": {},
   "source": [
    "## Training the Model for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3497836",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset,  # Ideally, a separate validation set\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
