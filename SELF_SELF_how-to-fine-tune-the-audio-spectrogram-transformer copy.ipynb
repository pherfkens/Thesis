{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46ee2202-9786-41e9-a41c-8f09192e8c12",
   "metadata": {},
   "source": [
    "# Fine-Tuning the Audio Spectrogram Transformer (AST) for Audio Classification\n",
    "\n",
    "This Jupyter Notebook provides a comprehensive guide for fine-tuning the Audio Spectrogram Transformer (AST) model on your own audio classification dataset using tools from the HuggingFace ecosystem and PyTorch. The notebook covers the entire workflow, including data loading, preprocessing, applying audio augmentations, configuring the model, and setting up the training process.\n",
    "\n",
    "**Published:** 30.07.2024  \n",
    "**Author:** Marius Steger  \n",
    "**Email:** [marius.steger@renumics.com](mailto:marius.steger@renumics.com)  \n",
    "**Organization:** [Renumics](https://renumics.com/)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694e0e96-8bdf-4147-b990-9a6df1205fb2",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Packages\n",
    "Before we start, install all the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f43d402-4217-44be-a44a-def251c3e77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers[torch] datasets[audio] audiomentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172c2b9c-0b13-4cc0-bd1f-8b7c92b73cea",
   "metadata": {},
   "source": [
    "## Step 2: Load Your Data in the Correct Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "99dafcdc-91e5-4bab-8ba1-699b79522238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, Audio, ClassLabel, Features, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1721b140-c598-4bb5-b8f8-17664e0d085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class labels\n",
    "#class_labels = ClassLabel(names=[\"bang\", \"dog_bark\"])\n",
    "\n",
    "# Define features with audio and label columns\n",
    "#features = Features({\n",
    "#    \"audio\": Audio(),\n",
    "#    \"labels\": class_labels\n",
    "#})\n",
    "\n",
    "# Load data (example with a dictionary)\n",
    "#dataset = Dataset.from_dict({\n",
    "#    \"audio\": [\"/audio/fold1/7061-6-0-0.wav\", \"/audio/fold1/7383-3-0-0.wav\"],\n",
    "#    \"labels\": [0, 1],\n",
    "#}, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d33f38c5-6b4a-4fd0-8398-ad08a4ffe011",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-existing dataset from the HuggingFace Hub\n",
    "esc50 = load_dataset(\"ashraq/esc50\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "72452d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(esc50[1342]['audio']['array'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f52267-4482-4d68-bd26-c8a9b084fbfa",
   "metadata": {},
   "source": [
    "## Step 3: Preprocess the Audio Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71680e4d-cbfa-4a7d-8a2e-abaff8d5cb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import Audio, ClassLabel\n",
    "from transformers import ASTFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "faafe166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>chirping_birds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36</td>\n",
       "      <td>vacuum_cleaner</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target        category\n",
       "0       0             dog\n",
       "1      14  chirping_birds\n",
       "2      36  vacuum_cleaner"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['dog',\n",
       " 'rooster',\n",
       " 'pig',\n",
       " 'cow',\n",
       " 'frog',\n",
       " 'cat',\n",
       " 'hen',\n",
       " 'insects',\n",
       " 'sheep',\n",
       " 'crow',\n",
       " 'rain',\n",
       " 'sea_waves',\n",
       " 'crackling_fire',\n",
       " 'crickets',\n",
       " 'chirping_birds',\n",
       " 'water_drops',\n",
       " 'wind',\n",
       " 'pouring_water',\n",
       " 'toilet_flush',\n",
       " 'thunderstorm',\n",
       " 'crying_baby',\n",
       " 'sneezing',\n",
       " 'clapping',\n",
       " 'breathing',\n",
       " 'coughing',\n",
       " 'footsteps',\n",
       " 'laughing',\n",
       " 'brushing_teeth',\n",
       " 'snoring',\n",
       " 'drinking_sipping',\n",
       " 'door_wood_knock',\n",
       " 'mouse_click',\n",
       " 'keyboard_typing',\n",
       " 'door_wood_creaks',\n",
       " 'can_opening',\n",
       " 'washing_machine',\n",
       " 'vacuum_cleaner',\n",
       " 'clock_alarm',\n",
       " 'clock_tick',\n",
       " 'glass_breaking',\n",
       " 'helicopter',\n",
       " 'chainsaw',\n",
       " 'siren',\n",
       " 'car_horn',\n",
       " 'engine',\n",
       " 'train',\n",
       " 'church_bells',\n",
       " 'airplane',\n",
       " 'fireworks',\n",
       " 'hand_saw']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get target value - class name mappings\n",
    "df = esc50.select_columns([\"target\", \"category\"]).to_pandas()\n",
    "class_names = df.iloc[np.unique(df[\"target\"], return_index=True)[1]][\"category\"].to_list()\n",
    "display(df.head(3))\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4d2397ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|██████████| 2000/2000 [00:01<00:00, 1130.77 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(80000,)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cast target and audio column\n",
    "esc50 = esc50.cast_column(\"target\", ClassLabel(names=class_names))\n",
    "esc50 = esc50.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "esc50[1342]['audio']['array'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c593aecc-06c6-4220-b0dd-0463df5bead4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# rename the target feature\n",
    "esc50 = esc50.rename_column(\"target\", \"labels\")\n",
    "num_labels = len(np.unique(esc50[\"labels\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e7b81944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(np.array_equal(esc50[0]['audio']['array'], array2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "33fcaf24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80000,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "esc50[1342]['audio']['array'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7be7bc9e-5ba5-4289-9c28-71b82ddf9137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pretrained model and instantiate the feature extractor\n",
    "pretrained_model = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "feature_extractor = ASTFeatureExtractor.from_pretrained(pretrained_model)\n",
    "model_input_name = feature_extractor.model_input_names[0]\n",
    "SAMPLING_RATE = feature_extractor.sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e636629e-6202-4a0c-a090-50023a6a829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_audio(batch):\n",
    "    wavs = [audio[\"array\"] for audio in batch[\"input_values\"]]\n",
    "    inputs = feature_extractor(wavs, sampling_rate=SAMPLING_RATE, return_tensors=\"pt\")\n",
    "    return {model_input_name: inputs.get(model_input_name), \"labels\": list(batch[\"labels\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "201eb68a-da4e-41ba-aa9f-20361bd1b26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the esc50 train split for this tutorial on how to fine-tune the AST Model\n",
    "dataset = esc50\n",
    "label2id = dataset.features[\"labels\"]._str2int  # we add the mapping from INTs to STRINGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "83879a8a-a8f0-4a00-9ed5-63b2bd721d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training data\n",
    "if \"test\" not in dataset:\n",
    "    dataset = dataset.train_test_split(\n",
    "        test_size=0.2, shuffle=True, seed=0, stratify_by_column=\"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7f1dafc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['filename', 'fold', 'labels', 'category', 'esc10', 'src_file', 'take', 'audio'],\n",
       "        num_rows: 1600\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['filename', 'fold', 'labels', 'category', 'esc10', 'src_file', 'take', 'audio'],\n",
       "        num_rows: 400\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54d506d-6908-478d-97d1-39f159940c73",
   "metadata": {},
   "source": [
    "## Step 4: Add Audio Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b43c7e36-e564-436d-958b-15b05c3bb94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from audiomentations import Compose, AddGaussianSNR, GainTransition, Gain, ClippingDistortion, TimeStretch, PitchShift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0c394b1-1900-411a-83d7-a1132bd3d2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define audio augmentations\n",
    "audio_augmentations = Compose([\n",
    "    AddGaussianSNR(min_snr_db=10, max_snr_db=20),\n",
    "    Gain(min_gain_db=-6, max_gain_db=6),\n",
    "    GainTransition(min_gain_db=-6, max_gain_db=6, min_duration=0.01, max_duration=0.3, duration_unit=\"fraction\"),\n",
    "    ClippingDistortion(min_percentile_threshold=0, max_percentile_threshold=30, p=0.5),\n",
    "    TimeStretch(min_rate=0.8, max_rate=1.2),\n",
    "    PitchShift(min_semitones=-4, max_semitones=4),\n",
    "], p=0.8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "597c8e0e-1172-4dfd-90e8-d715b9eae1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing with augmentations\n",
    "def preprocess_audio_with_transforms(batch):\n",
    "    wavs = [audio_augmentations(audio[\"array\"], sample_rate=SAMPLING_RATE) for audio in batch[\"input_values\"]]\n",
    "    inputs = feature_extractor(wavs, sampling_rate=SAMPLING_RATE, return_tensors=\"pt\")\n",
    "    return {model_input_name: inputs.get(model_input_name), \"labels\": list(batch[\"labels\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bad960a-13b5-43e9-89b9-902489af986c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=feature_extractor.sampling_rate))\n",
    "dataset = dataset.rename_column(\"audio\", \"input_values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2fea2fb-d150-4e8c-be60-3500adcea678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated mean and std: -3.3504603 4.387065\n"
     ]
    }
   ],
   "source": [
    "# calculate values for normalization\n",
    "feature_extractor.do_normalize = False  # we set normalization to False in order to calculate the mean + std of the dataset\n",
    "mean = []\n",
    "std = []\n",
    "\n",
    "# we use the transformation w/o augmentation on the training dataset to calculate the mean + std\n",
    "dataset[\"train\"].set_transform(preprocess_audio, output_all_columns=False)\n",
    "for i, (audio_input, labels) in enumerate(dataset[\"train\"]):\n",
    "    cur_mean = torch.mean(dataset[\"train\"][i][audio_input])\n",
    "    cur_std = torch.std(dataset[\"train\"][i][audio_input])\n",
    "    mean.append(cur_mean)\n",
    "    std.append(cur_std)\n",
    "\n",
    "feature_extractor.mean = np.mean(mean)\n",
    "feature_extractor.std = np.mean(std)\n",
    "feature_extractor.do_normalize = True\n",
    "\n",
    "print(\"Calculated mean and std:\", feature_extractor.mean, feature_extractor.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5898b2d0-0bcc-463e-8c27-96e7818111fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transforms\n",
    "dataset[\"train\"].set_transform(preprocess_audio_with_transforms, output_all_columns=False)\n",
    "dataset[\"test\"].set_transform(preprocess_audio, output_all_columns=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b5447f-5add-46a1-8425-c094131cafd2",
   "metadata": {},
   "source": [
    "## Step 5: Configure and Initialize the AST for Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e49f002d-03b7-4e51-b6cd-1f4415f8b549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from transformers import ASTConfig, ASTForAudioClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d484ed33-d35d-4d3b-9b89-df37a0675ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration from the pretrained model\n",
    "config = ASTConfig.from_pretrained(pretrained_model)\n",
    "config.num_labels = num_labels\n",
    "config.label2id = label2id\n",
    "config.id2label = {v: k for k, v in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a400322c-e56a-4f95-8668-9307e32ce3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 and are newly initialized because the shapes did not match:\n",
      "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([50]) in the model instantiated\n",
      "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([50, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model with the updated configuration\n",
    "model = ASTForAudioClassification.from_pretrained(pretrained_model, config=config, ignore_mismatched_sizes=True)\n",
    "model.init_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee50f12a-47bc-4d31-93c1-989911e97037",
   "metadata": {},
   "source": [
    "### Setup Metrics and Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "866e564f-11ae-40ec-acb8-febee019ebe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./runs/ast_classifier\",\n",
    "    logging_dir=f\"./logs/ast_classifier\",\n",
    "    report_to=\"tensorboard\",\n",
    "    learning_rate=5e-5,  # LEARNING RATE\n",
    "    push_to_hub=False,\n",
    "    num_train_epochs=10,  # EPOCHS\n",
    "    per_device_train_batch_size=8,  # BATCH SIZE\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_steps=1,\n",
    "    save_steps=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",  # eval_+metric ist utilized\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abb0e7cc-e3c0-46d2-8271-fa0eac9442c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metrics\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "AVERAGE = \"macro\" if config.num_labels > 2 else \"binary\"\n",
    "\n",
    "# setup metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    # get predictions and scores\n",
    "    logits = eval_pred.predictions\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "\n",
    "    # compute metrics\n",
    "    metrics = accuracy.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "    metrics.update(precision.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))\n",
    "    metrics.update(recall.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))\n",
    "    metrics.update(f1.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f469b71-00e8-4f4e-ab24-434ebcda243d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,  # we use our configured training arguments\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,  # we the metrics function from above\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7af0820e-fa77-4f44-95ee-612896374c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages/audiomentations/core/transforms_interface.py:62: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2000/2000 12:14, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.064900</td>\n",
       "      <td>0.557234</td>\n",
       "      <td>0.852500</td>\n",
       "      <td>0.888413</td>\n",
       "      <td>0.852500</td>\n",
       "      <td>0.844813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.470500</td>\n",
       "      <td>0.292247</td>\n",
       "      <td>0.902500</td>\n",
       "      <td>0.918432</td>\n",
       "      <td>0.902500</td>\n",
       "      <td>0.902523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.402400</td>\n",
       "      <td>0.295535</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.935807</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.921613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.402100</td>\n",
       "      <td>0.304958</td>\n",
       "      <td>0.927500</td>\n",
       "      <td>0.938123</td>\n",
       "      <td>0.927500</td>\n",
       "      <td>0.926955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.229600</td>\n",
       "      <td>0.268698</td>\n",
       "      <td>0.915000</td>\n",
       "      <td>0.925730</td>\n",
       "      <td>0.915000</td>\n",
       "      <td>0.911636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.225600</td>\n",
       "      <td>0.191082</td>\n",
       "      <td>0.947500</td>\n",
       "      <td>0.953143</td>\n",
       "      <td>0.947500</td>\n",
       "      <td>0.947632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.178400</td>\n",
       "      <td>0.268304</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.938053</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.925158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.080200</td>\n",
       "      <td>0.175834</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.956268</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.950184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.082000</td>\n",
       "      <td>0.141045</td>\n",
       "      <td>0.957500</td>\n",
       "      <td>0.962000</td>\n",
       "      <td>0.957500</td>\n",
       "      <td>0.957410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>0.133586</td>\n",
       "      <td>0.955000</td>\n",
       "      <td>0.959500</td>\n",
       "      <td>0.955000</td>\n",
       "      <td>0.955100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 1024}\n",
      "/home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages/audiomentations/core/transforms_interface.py:62: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 1024}\n",
      "/home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages/audiomentations/core/transforms_interface.py:62: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 1024}\n",
      "/home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages/audiomentations/core/transforms_interface.py:62: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 1024}\n",
      "/home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages/audiomentations/core/transforms_interface.py:62: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 1024}\n",
      "/home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages/audiomentations/core/transforms_interface.py:62: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 1024}\n",
      "/home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages/audiomentations/core/transforms_interface.py:62: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 1024}\n",
      "/home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages/audiomentations/core/transforms_interface.py:62: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 1024}\n",
      "/home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages/audiomentations/core/transforms_interface.py:62: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 1024}\n",
      "/home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages/audiomentations/core/transforms_interface.py:62: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 1024}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 1024}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2000, training_loss=0.4032806022465229, metrics={'train_runtime': 734.9587, 'train_samples_per_second': 21.77, 'train_steps_per_second': 2.721, 'total_flos': 1.084989898752e+18, 'train_loss': 0.4032806022465229, 'epoch': 10.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start a training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d12f99-709e-46a6-90f1-f982dcb27e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
