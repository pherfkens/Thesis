{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46ee2202-9786-41e9-a41c-8f09192e8c12",
   "metadata": {},
   "source": [
    "# Fine-Tuning the Audio Spectrogram Transformer (AST) for Audio Classification\n",
    "\n",
    "This Jupyter Notebook provides a comprehensive guide for fine-tuning the Audio Spectrogram Transformer (AST) model on your own audio classification dataset using tools from the HuggingFace ecosystem and PyTorch. The notebook covers the entire workflow, including data loading, preprocessing, applying audio augmentations, configuring the model, and setting up the training process.\n",
    "\n",
    "**Published:** 30.07.2024  \n",
    "**Author:** Marius Steger  \n",
    "**Email:** [marius.steger@renumics.com](mailto:marius.steger@renumics.com)  \n",
    "**Organization:** [Renumics](https://renumics.com/)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694e0e96-8bdf-4147-b990-9a6df1205fb2",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Packages\n",
    "Before we start, install all the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f43d402-4217-44be-a44a-def251c3e77f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2312878907.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpip install audiomentations\u001b[39m\n        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers[torch] datasets[audio]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c51d0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting audiomentations\n",
      "  Downloading audiomentations-0.40.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting numpy<2,>=1.22.0 (from audiomentations)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata (61 kB)\n",
      "Collecting numpy-minmax<1,>=0.3.0 (from audiomentations)\n",
      "  Downloading numpy_minmax-0.4.0-cp311-cp311-win_amd64.whl.metadata (4.3 kB)\n",
      "Collecting numpy-rms<1,>=0.4.2 (from audiomentations)\n",
      "  Downloading numpy_rms-0.5.0-cp311-cp311-win_amd64.whl.metadata (3.6 kB)\n",
      "Collecting librosa!=0.10.0,<0.11.0,>=0.8.0 (from audiomentations)\n",
      "  Downloading librosa-0.10.2.post1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting python-stretch<1,>=0.3.1 (from audiomentations)\n",
      "  Downloading python_stretch-0.3.1-cp311-cp311-win_amd64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: scipy<2,>=1.4 in c:\\users\\pepij\\onedrive - delft university of technology\\thesis\\workspace-thesis\\.venv\\lib\\site-packages (from audiomentations) (1.15.2)\n",
      "Requirement already satisfied: soxr<1.0.0,>=0.3.2 in c:\\users\\pepij\\onedrive - delft university of technology\\thesis\\workspace-thesis\\.venv\\lib\\site-packages (from audiomentations) (0.5.0.post1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\pepij\\onedrive - delft university of technology\\thesis\\workspace-thesis\\.venv\\lib\\site-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.0.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\pepij\\onedrive - delft university of technology\\thesis\\workspace-thesis\\.venv\\lib\\site-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.6.1)\n",
      "Requirement already satisfied: joblib>=0.14 in c:\\users\\pepij\\onedrive - delft university of technology\\thesis\\workspace-thesis\\.venv\\lib\\site-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\pepij\\onedrive - delft university of technology\\thesis\\workspace-thesis\\.venv\\lib\\site-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (5.2.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\pepij\\onedrive - delft university of technology\\thesis\\workspace-thesis\\.venv\\lib\\site-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.61.0)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in c:\\users\\pepij\\onedrive - delft university of technology\\thesis\\workspace-thesis\\.venv\\lib\\site-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in c:\\users\\pepij\\onedrive - delft university of technology\\thesis\\workspace-thesis\\.venv\\lib\\site-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.8.2)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in c:\\users\\pepij\\onedrive - delft university of technology\\thesis\\workspace-thesis\\.venv\\lib\\site-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (4.12.2)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in c:\\users\\pepij\\onedrive - delft university of technology\\thesis\\workspace-thesis\\.venv\\lib\\site-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\pepij\\onedrive - delft university of technology\\thesis\\workspace-thesis\\.venv\\lib\\site-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.1.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in c:\\users\\pepij\\onedrive - delft university of technology\\thesis\\workspace-thesis\\.venv\\lib\\site-packages (from numpy-minmax<1,>=0.3.0->audiomentations) (1.17.1)\n",
      "INFO: pip is looking at multiple versions of numpy-minmax to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting numpy-minmax<1,>=0.3.0 (from audiomentations)\n",
      "  Downloading numpy_minmax-0.3.1-cp311-cp311-win_amd64.whl.metadata (4.1 kB)\n",
      "INFO: pip is looking at multiple versions of numpy-rms to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting numpy-rms<1,>=0.4.2 (from audiomentations)\n",
      "  Downloading numpy_rms-0.4.2-cp311-cp311-win_amd64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: pycparser in c:\\users\\pepij\\onedrive - delft university of technology\\thesis\\workspace-thesis\\.venv\\lib\\site-packages (from cffi>=1.0.0->numpy-minmax<1,>=0.3.0->audiomentations) (2.22)\n",
      "Requirement already satisfied: packaging in c:\\users\\pepij\\onedrive - delft university of technology\\thesis\\workspace-thesis\\.venv\\lib\\site-packages (from lazy-loader>=0.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (24.2)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\users\\pepij\\onedrive - delft university of technology\\thesis\\workspace-thesis\\.venv\\lib\\site-packages (from numba>=0.51.0->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\pepij\\onedrive - delft university of technology\\thesis\\workspace-thesis\\.venv\\lib\\site-packages (from pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (4.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\pepij\\onedrive - delft university of technology\\thesis\\workspace-thesis\\.venv\\lib\\site-packages (from pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (2.32.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\pepij\\onedrive - delft university of technology\\thesis\\workspace-thesis\\.venv\\lib\\site-packages (from scikit-learn>=0.20.0->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pepij\\onedrive - delft university of technology\\thesis\\workspace-thesis\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pepij\\onedrive - delft university of technology\\thesis\\workspace-thesis\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pepij\\onedrive - delft university of technology\\thesis\\workspace-thesis\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pepij\\onedrive - delft university of technology\\thesis\\workspace-thesis\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (2025.1.31)\n",
      "Downloading audiomentations-0.40.0-py3-none-any.whl (83 kB)\n",
      "Downloading librosa-0.10.2.post1-py3-none-any.whl (260 kB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "Downloading numpy_minmax-0.3.1-cp311-cp311-win_amd64.whl (14 kB)\n",
      "Downloading numpy_rms-0.4.2-cp311-cp311-win_amd64.whl (13 kB)\n",
      "Downloading python_stretch-0.3.1-cp311-cp311-win_amd64.whl (98 kB)\n",
      "Installing collected packages: python-stretch, numpy, numpy-rms, numpy-minmax, librosa, audiomentations\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.3\n",
      "    Uninstalling numpy-2.1.3:\n",
      "      Successfully uninstalled numpy-2.1.3\n",
      "  Attempting uninstall: librosa\n",
      "    Found existing installation: librosa 0.11.0\n",
      "    Uninstalling librosa-0.11.0:\n",
      "      Successfully uninstalled librosa-0.11.0\n",
      "Successfully installed audiomentations-0.40.0 librosa-0.10.2.post1 numpy-1.26.4 numpy-minmax-0.3.1 numpy-rms-0.4.2 python-stretch-0.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\~-mpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\~-mpy'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "pip install audiomentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172c2b9c-0b13-4cc0-bd1f-8b7c92b73cea",
   "metadata": {},
   "source": [
    "## Step 2: Load Your Data in the Correct Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99dafcdc-91e5-4bab-8ba1-699b79522238",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, Audio, ClassLabel, Features, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1721b140-c598-4bb5-b8f8-17664e0d085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class labels\n",
    "#class_labels = ClassLabel(names=[\"bang\", \"dog_bark\"])\n",
    "\n",
    "# Define features with audio and label columns\n",
    "#features = Features({\n",
    "#    \"audio\": Audio(),\n",
    "#    \"labels\": class_labels\n",
    "#})\n",
    "\n",
    "# Load data (example with a dictionary)\n",
    "#dataset = Dataset.from_dict({\n",
    "#    \"audio\": [\"/audio/fold1/7061-6-0-0.wav\", \"/audio/fold1/7383-3-0-0.wav\"],\n",
    "#    \"labels\": [0, 1],\n",
    "#}, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d33f38c5-6b4a-4fd0-8398-ad08a4ffe011",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-existing dataset from the HuggingFace Hub\n",
    "esc50 = load_dataset(\"ashraq/esc50\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f52267-4482-4d68-bd26-c8a9b084fbfa",
   "metadata": {},
   "source": [
    "## Step 3: Preprocess the Audio Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "71680e4d-cbfa-4a7d-8a2e-abaff8d5cb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import Audio, ClassLabel\n",
    "from transformers import ASTFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c593aecc-06c6-4220-b0dd-0463df5bead4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get target value - class name mappings\n",
    "df = esc50.select_columns([\"target\", \"category\"]).to_pandas()\n",
    "class_names = df.iloc[np.unique(df[\"target\"], return_index=True)[1]][\"category\"].to_list()\n",
    "\n",
    "# cast target and audio column\n",
    "esc50 = esc50.cast_column(\"target\", ClassLabel(names=class_names))\n",
    "esc50 = esc50.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "# rename the target feature\n",
    "esc50 = esc50.rename_column(\"target\", \"labels\")\n",
    "num_labels = len(np.unique(esc50[\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7be7bc9e-5ba5-4289-9c28-71b82ddf9137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pretrained model and instantiate the feature extractor\n",
    "pretrained_model = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "feature_extractor = ASTFeatureExtractor.from_pretrained(pretrained_model)\n",
    "model_input_name = feature_extractor.model_input_names[0]\n",
    "SAMPLING_RATE = feature_extractor.sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e636629e-6202-4a0c-a090-50023a6a829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_audio(batch):\n",
    "    print(batch)\n",
    "    wavs = [audio[\"array\"] for audio in batch[\"input_values\"]]\n",
    "    inputs = feature_extractor(wavs, sampling_rate=SAMPLING_RATE, return_tensors=\"pt\")\n",
    "    return {model_input_name: inputs.get(model_input_name), \"labels\": list(batch[\"labels\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "201eb68a-da4e-41ba-aa9f-20361bd1b26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the esc50 train split for this tutorial on how to fine-tune the AST Model\n",
    "dataset = esc50\n",
    "label2id = dataset.features[\"labels\"]._str2int  # we add the mapping from INTs to STRINGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "83879a8a-a8f0-4a00-9ed5-63b2bd721d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training data\n",
    "if \"test\" not in dataset:\n",
    "    dataset = dataset.train_test_split(\n",
    "        test_size=0.2, shuffle=True, seed=0, stratify_by_column=\"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ee3bd222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['filename', 'fold', 'labels', 'category', 'esc10', 'src_file', 'take', 'audio'],\n",
       "        num_rows: 1600\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['filename', 'fold', 'labels', 'category', 'esc10', 'src_file', 'take', 'audio'],\n",
       "        num_rows: 400\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54d506d-6908-478d-97d1-39f159940c73",
   "metadata": {},
   "source": [
    "## Step 4: Add Audio Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b43c7e36-e564-436d-958b-15b05c3bb94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from audiomentations import Compose, AddGaussianSNR, GainTransition, Gain, ClippingDistortion, TimeStretch, PitchShift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0c394b1-1900-411a-83d7-a1132bd3d2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define audio augmentations\n",
    "audio_augmentations = Compose([\n",
    "    AddGaussianSNR(min_snr_db=10, max_snr_db=20),\n",
    "    Gain(min_gain_db=-6, max_gain_db=6),\n",
    "    GainTransition(min_gain_db=-6, max_gain_db=6, min_duration=0.01, max_duration=0.3, duration_unit=\"fraction\"),\n",
    "    ClippingDistortion(min_percentile_threshold=0, max_percentile_threshold=30, p=0.5),\n",
    "    TimeStretch(min_rate=0.8, max_rate=1.2),\n",
    "    PitchShift(min_semitones=-4, max_semitones=4),\n",
    "], p=0.8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "597c8e0e-1172-4dfd-90e8-d715b9eae1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing with augmentations\n",
    "def preprocess_audio_with_transforms(batch):\n",
    "    wavs = [audio_augmentations(audio[\"array\"], sample_rate=SAMPLING_RATE) for audio in batch[\"input_values\"]]\n",
    "    inputs = feature_extractor(wavs, sampling_rate=SAMPLING_RATE, return_tensors=\"pt\")\n",
    "    return {model_input_name: inputs.get(model_input_name), \"labels\": list(batch[\"labels\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4bad960a-13b5-43e9-89b9-902489af986c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=feature_extractor.sampling_rate))\n",
    "dataset = dataset.rename_column(\"audio\", \"input_values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2fea2fb-d150-4e8c-be60-3500adcea678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated mean and std: -3.3504603 4.387065\n"
     ]
    }
   ],
   "source": [
    "# calculate values for normalization\n",
    "feature_extractor.do_normalize = False  # we set normalization to False in order to calculate the mean + std of the dataset\n",
    "mean = []\n",
    "std = []\n",
    "\n",
    "# we use the transformation w/o augmentation on the training dataset to calculate the mean + std\n",
    "dataset[\"train\"].set_transform(preprocess_audio, output_all_columns=False)\n",
    "for i, (audio_input, labels) in enumerate(dataset[\"train\"]):\n",
    "    cur_mean = torch.mean(dataset[\"train\"][i][audio_input])\n",
    "    cur_std = torch.std(dataset[\"train\"][i][audio_input])\n",
    "    mean.append(cur_mean)\n",
    "    std.append(cur_std)\n",
    "\n",
    "feature_extractor.mean = np.mean(mean)\n",
    "feature_extractor.std = np.mean(std)\n",
    "feature_extractor.do_normalize = True\n",
    "\n",
    "print(\"Calculated mean and std:\", feature_extractor.mean, feature_extractor.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5898b2d0-0bcc-463e-8c27-96e7818111fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transforms\n",
    "dataset[\"train\"].set_transform(preprocess_audio_with_transforms, output_all_columns=False)\n",
    "dataset[\"test\"].set_transform(preprocess_audio, output_all_columns=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b6fc580c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['filename', 'fold', 'labels', 'category', 'esc10', 'src_file', 'take', 'input_values'],\n",
       "        num_rows: 1600\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['filename', 'fold', 'labels', 'category', 'esc10', 'src_file', 'take', 'input_values'],\n",
       "        num_rows: 400\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b5447f-5add-46a1-8425-c094131cafd2",
   "metadata": {},
   "source": [
    "## Step 5: Configure and Initialize the AST for Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e49f002d-03b7-4e51-b6cd-1f4415f8b549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from transformers import ASTConfig, ASTForAudioClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d484ed33-d35d-4d3b-9b89-df37a0675ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration from the pretrained model\n",
    "config = ASTConfig.from_pretrained(pretrained_model)\n",
    "config.num_labels = num_labels\n",
    "config.label2id = label2id\n",
    "config.id2label = {v: k for k, v in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a400322c-e56a-4f95-8668-9307e32ce3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 and are newly initialized because the shapes did not match:\n",
      "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([50]) in the model instantiated\n",
      "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([50, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model with the updated configuration\n",
    "model = ASTForAudioClassification.from_pretrained(pretrained_model, config=config, ignore_mismatched_sizes=True)\n",
    "model.init_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee50f12a-47bc-4d31-93c1-989911e97037",
   "metadata": {},
   "source": [
    "### Setup Metrics and Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866e564f-11ae-40ec-acb8-febee019ebe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAST TRAINING SETUP\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./runs/ast_classifier_quick\",\n",
    "    logging_dir=\"./logs/ast_classifier_quick\",\n",
    "    report_to=\"none\",  # disable TensorBoard if not needed\n",
    "    learning_rate=1e-4,  # optionally increase learning rate for faster convergence\n",
    "    push_to_hub=False,\n",
    "    num_train_epochs=1,  # reduce epochs\n",
    "    per_device_train_batch_size=2,  # reduce batch size\n",
    "    eval_strategy=\"steps\",  # more frequent eval if needed\n",
    "    save_strategy=\"no\",  # skip saving for fast testing\n",
    "    eval_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=5,\n",
    "    load_best_model_at_end=False,  # skip loading best\n",
    "    disable_tqdm=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "abb0e7cc-e3c0-46d2-8271-fa0eac9442c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metrics\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "AVERAGE = \"macro\" if config.num_labels > 2 else \"binary\"\n",
    "\n",
    "# setup metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    # get predictions and scores\n",
    "    logits = eval_pred.predictions\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "\n",
    "    # compute metrics\n",
    "    metrics = accuracy.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "    metrics.update(precision.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))\n",
    "    metrics.update(recall.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))\n",
    "    metrics.update(f1.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1f469b71-00e8-4f4e-ab24-434ebcda243d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,  # we use our configured training arguments\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,  # we the metrics function from above\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7af0820e-fa77-4f44-95ee-612896374c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  4/800 00:41 < 4:31:58, 0.05 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# start a training\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2241\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2239\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2240\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2245\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2548\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2541\u001b[39m context = (\n\u001b[32m   2542\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2543\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2544\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2545\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2546\u001b[39m )\n\u001b[32m   2547\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2548\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2551\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2552\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2553\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2554\u001b[39m ):\n\u001b[32m   2555\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2556\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3740\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3737\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   3738\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3740\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3742\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\accelerate\\accelerator.py:2359\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2357\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2358\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2359\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\Workspace-Thesis\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# start a training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d12f99-709e-46a6-90f1-f982dcb27e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
