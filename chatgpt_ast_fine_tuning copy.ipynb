{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, Audio, ClassLabel, Features, load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "# Load metadata CSV (assuming dataset includes a CSV file linking audio to perceptual attributes)\n",
    "metadata = pd.read_excel(r\"C:\\Users\\pepij\\Downloads\\noorderplantsoen.xlsx\")\n",
    "\n",
    "# Preview dataset\n",
    "display(metadata.head())\n",
    "\n",
    "metadata[\"audio_path\"] = metadata[\"GroupID\"].apply(lambda x: r\"C:\\Users\\pepij\\OneDrive - Delft University of Technology\\THESIS\\data\\WAV_Groningen_1\\WAV_Groningen_1\\Noorderplantsoen\\NP\" + x[2:] + \".wav\")\n",
    "\n",
    "# Keep only rows where the file exists\n",
    "metadata = metadata[metadata[\"audio_path\"].apply(os.path.exists)]\n",
    "\n",
    "# Reset index after filtering\n",
    "metadata.reset_index(drop=True, inplace=True)\n",
    "\n",
    "metadata = metadata[['GroupID', 'pleasant', 'chaotic', 'vibrant', 'uneventful', 'calm', 'annoying', 'eventful', 'monotonous', 'audio_path']]\n",
    "\n",
    "columns_to_convert = [\n",
    "    \"pleasant\", \"chaotic\", \"vibrant\", \"uneventful\", \n",
    "    \"calm\", \"annoying\", \"eventful\", \"monotonous\"\n",
    "]\n",
    "\n",
    "metadata[columns_to_convert] = metadata[columns_to_convert].astype(float).values\n",
    "import numpy as np\n",
    "from datasets import Audio, ClassLabel\n",
    "from transformers import ASTFeatureExtractor\n",
    "\n",
    "def load_audio(row):\n",
    "    waveform, sr = torchaudio.load(row['audio_path'])\n",
    "    return {\n",
    "        'filename': f\"{row['GroupID']}.wav\",\n",
    "        'labels': {k: row[k] for k in ['pleasant', 'chaotic', 'vibrant', 'uneventful', 'calm', 'annoying', 'eventful', 'monotonous']},\n",
    "        # 'labels': [\n",
    "        #     row['pleasant'], row['chaotic'], row['vibrant'], row['uneventful'],\n",
    "        #     row['calm'], row['annoying'], row['eventful'], row['monotonous']\n",
    "        # ],\n",
    "        'audio': {\n",
    "            'path': row['audio_path'],\n",
    "            'array': waveform.squeeze().numpy().reshape(-1),\n",
    "            'sampling_rate': sr\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Step 1: Apply transformation to entire DataFrame\n",
    "records = metadata.apply(load_audio, axis=1)\n",
    "\n",
    "# Step 2: Apply to rows and collect results as list of dicts\n",
    "records = [load_audio(row) for _, row in metadata.iterrows()]\n",
    "\n",
    "# Step 3: Create a new DataFrame directly\n",
    "new_df = pd.DataFrame(records)\n",
    "dataset = Dataset.from_pandas(new_df)\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "dataset[0]\n",
    "dataset[0]['audio']['array'].shape\n",
    "# dataset[0]\n",
    "# De oude format als je eerst numpy array zou maken maar is dus niet nodig zie hierboven\n",
    "# Define the pretrained model and instantiate the feature extractor\n",
    "pretrained_model = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "feature_extractor = ASTFeatureExtractor.from_pretrained(pretrained_model)\n",
    "model_input_name = feature_extractor.model_input_names[0]\n",
    "SAMPLING_RATE = feature_extractor.sampling_rate\n",
    "# Preprocessing function\n",
    "def preprocess_audio(batch):\n",
    "    wavs = [audio[\"array\"] for audio in batch[\"input_values\"]]\n",
    "    inputs = feature_extractor(wavs, sampling_rate=SAMPLING_RATE, return_tensors=\"pt\")\n",
    "    return {model_input_name: inputs.get(model_input_name), \"labels\": list(batch[\"labels\"])}\n",
    "# split training data\n",
    "if \"test\" not in dataset:\n",
    "    dataset = dataset.train_test_split(\n",
    "        test_size=0.2, shuffle=True, seed=0)\n",
    "dataset\n",
    "# Apply transforms\n",
    "dataset[\"train\"].set_transform(preprocess_audio, output_all_columns=False)\n",
    "dataset[\"test\"].set_transform(preprocess_audio, output_all_columns=False)\n",
    "import evaluate\n",
    "from transformers import ASTConfig, ASTForAudioClassification, TrainingArguments, Trainer\n",
    "# Load configuration from the pretrained model\n",
    "config = ASTConfig.from_pretrained(pretrained_model)\n",
    "\n",
    "# Modify the model's final layer for regression (8 outputs)\n",
    "model = ASTForAudioClassification.from_pretrained(pretrained_model, config=config, ignore_mismatched_sizes=True)\n",
    "model.classifier = nn.Linear(config.hidden_size, 8)\n",
    "model.init_weights()\n",
    "# Configure training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./runs/ast_regressor\",\n",
    "    logging_dir=\"./logs/ast_regressor\",\n",
    "    report_to=\"tensorboard\",\n",
    "    learning_rate=5e-5,  # Learning rate\n",
    "    push_to_hub=False,\n",
    "    num_train_epochs=3,  # Number of epochs\n",
    "    per_device_train_batch_size=8,  # Batch size\n",
    "    eval_strategy=\"epoch\",  # Evaluate after each epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rmse\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=20,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "mse = evaluate.load(\"mse\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits = eval_pred.predictions\n",
    "    labels = eval_pred.label_ids\n",
    "\n",
    "    mse_value = mse.compute(predictions=logits, references=labels)[\"mse\"]\n",
    "    rmse_value = np.sqrt(mse_value)  # Compute RMSE\n",
    "\n",
    "    return {\"mse\": mse_value, \"rmse\": rmse_value}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
